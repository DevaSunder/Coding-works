{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8061761,"sourceType":"datasetVersion","datasetId":4755549},{"sourceId":8153205,"sourceType":"datasetVersion","datasetId":4822379},{"sourceId":8197753,"sourceType":"datasetVersion","datasetId":4855992},{"sourceId":8252954,"sourceType":"datasetVersion","datasetId":4897197}],"dockerImageVersionId":30637,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"### UTILITIES\nfrom keras.utils import to_categorical\nimport pickle\nimport sys\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nfrom keras.datasets import cifar10\nfrom PIL import Image\nimport os\nimport numpy as np\n\n\n\"\"\"\ndef load_images_from_folder(folder_path, image_shape):\n    images = []\n    labels = []\n\n    for label, class_folder in enumerate(sorted(os.listdir(folder_path))):\n        class_path = os.path.join(folder_path, class_folder)\n        for filename in os.listdir(class_path):\n            img_path = os.path.join(class_path, filename)\n            img = load_image(img_path, image_shape)\n            images.append(img)\n            labels.append(label)\n\n    images = np.array(images)\n    labels = np.array(labels)\n    return images, labels\n\nimport numpy as np\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\ndef load_dataset(batch_size, num_classes, epochs):\n    # Load CIFAR-10 dataset\n    (x_train_full, y_train_full), (x_test, y_test) = cifar10.load_data()\n\n    # Normalize images (convert to float32 and scale to [0,1])\n    x_train_full = x_train_full.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\n\n    # Convert labels to one-hot encoding\n    y_train_full = to_categorical(y_train_full, num_classes)\n    y_test = to_categorical(y_test, num_classes)\n\n    # Split training data into 40,000 for training and 10,000 for validation\n    x_train, x_val, y_train, y_val = train_test_split(\n        x_train_full, y_train_full, test_size=0.2, random_state=42\n    )\n\n    dataset = {\n        'batch_size': batch_size,\n        'num_classes': num_classes,\n        'epochs': epochs,\n        'x_train': x_train,\n        'y_train': y_train,\n        'x_val': x_val,\n        'y_val': y_val,\n        'x_test': x_test,  \n        'y_test': y_test\n    }\n\n    return dataset\n\"\"\"\n\ndef save_network(network):\n    #object_file = open(network.name + '.obj', 'wb')\n    #pickle.dump(network, object_file)\n    #tf.keras.models.save_model(network, network.name)\n\n    model_path = network.name + '_model.h5'\n    tf.keras.models.save_model(network.model, model_path)\n\n    # Save the rest of the network information\n    network_info = {\n        'name': network.name,\n        'block_list': network.block_list,\n        'fitness': network.fitness\n    }\n    network_info_path = network.name + '_info.pkl'\n    with open(network_info_path, 'wb') as info_file:\n        pickle.dump(network_info, info_file)\n\n\ndef load_network(name):\n    model_path = name + '_model.h5'\n    loaded_model = tf.keras.models.load_model(model_path)\n\n    # Load the network information\n    info_path = name + '_info.pkl'\n    with open(info_path, 'rb') as info_file:\n        network_info = pickle.load(info_file)\n\n    # Create a new Network instance\n    loaded_network = Network(0)  # Update with appropriate 'it' value\n\n    # Set the attributes of the loaded network\n    loaded_network.name = network_info['name']\n    loaded_network.block_list = network_info['block_list']\n    loaded_network.fitness = network_info['fitness']\n    loaded_network.model = loaded_model\n\n    return loaded_network\n\n\n\ndef order_indexes(self):\n    i = 0\n    for block in self.block_list:\n        block.index = i\n        i += 1\n\n\ndef plot_training(history):                                           # plot diagnostic learning curves\n    plt.figure(figsize=[8, 6])  # accuracy curves\n    plt.plot(history.history['accuracy'], 'r', linewidth=3.0)\n    plt.plot(history.history['val_accuracy'], 'b', linewidth=3.0)  # <-- Change 'val_acc' to 'val_accuracy'\n    plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=18)\n    plt.xlabel('Epochs ', fontsize=16)\n    plt.ylabel('Accuracy', fontsize=16)\n    plt.title('Accuracy Curves', fontsize=16)\n\n    filename = sys.argv[0].split('/')[-1]\n    plt.savefig(filename + '_acc_plot.png')\n    plt.close()\n\n\n\ndef plot_statistics(stats):\n    plt.figure(figsize=[8, 6])\t\t\t\t\t\t\t\t\t\t\t# fitness curves\n    plt.plot([s[0] for s in stats], 'r', linewidth=3.0)\n    plt.plot([stats[0][0]] * len(stats), 'b', linewidth=3.0)\n    plt.legend(['BestFitness', 'InitialFitness'], fontsize=18)\n    plt.xlabel('Generations', fontsize=16)\n    plt.ylabel('FitnessValue', fontsize=16)\n    plt.title('Fitness Curve', fontsize=16)\n    filename = sys.argv[0].split('/')[-1]\n    plt.savefig(filename + '_fitness_plot.png')\n\n    plt.figure(figsize=[8, 6])\t\t\t\t\t\t\t\t\t\t\t# parameters curves\n    plt.plot([s[1] for s in stats], 'r', linewidth=3.0)\n    plt.plot([stats[0][1]] * len(stats), 'b', linewidth=3.0)\n    plt.legend(['BestParamsNum', 'InitialParamsNum'], fontsize=18)\n    plt.xlabel('Generations', fontsize=16)\n    plt.ylabel('ParamsNum', fontsize=16)\n    plt.title('Parameters Curve', fontsize=16)\n    filename = sys.argv[0].split('/')[-1]\n    plt.savefig(filename + '_params_plot.png')\n    plt.close()\n","metadata":{"execution":{"iopub.status.busy":"2025-02-26T20:48:47.456560Z","iopub.execute_input":"2025-02-26T20:48:47.456871Z","iopub.status.idle":"2025-02-26T20:48:47.471752Z","shell.execute_reply.started":"2025-02-26T20:48:47.456839Z","shell.execute_reply":"2025-02-26T20:48:47.470782Z"},"trusted":true},"outputs":[],"execution_count":46},{"cell_type":"code","source":"#(Block number,position)\n# initial - 0, # middle -1, Final -2\n\n# INOUT\nimport os\ndef compute_parent(dataset):\n    if os.path.isfile('parent_0.h5'):\n        daddy = load_network('parent_0')\n        model = tf.keras.models.load_model('parent_0.h5')\n        print(\"Loading parent_0\")\n        print(\"SUMMARY OF\", daddy.name)\n        print(model.summary())\n        print(\"FITNESS:\", daddy.fitness)\n        return daddy\n\n    daddy = Network(0)\n    \n    \n    #INI BLOCK\n    layerList1 = [\n        Convolutional(filters=32, filter_size=(3, 3), stride_size=(1, 1), padding='same',\n                      input_shape=dataset['x_train'].shape[1:]),\n        Pooling(pool_size=(2, 2), stride_size=(2, 2), padding='same')\n    ]\n    layerList2 = [\n        Dropout(0.25)\n    ]\n    daddy.block_list.append(Block(0, 0, layerList1, layerList2))\n    \n    #MIDDLE BLOCK 1\n    layerList1 = [\n        Convolutional(filters=32, filter_size=(3, 3), stride_size=(1, 1), padding='same',\n                      input_shape=dataset['x_train'].shape[1:]),\n        Pooling(pool_size=(2, 2), stride_size=(2, 2), padding='same')\n    ]\n    layerList2 = [\n        Dropout(0.25)\n    ]\n    daddy.block_list.append(Block(1, 1, layerList1, layerList2))\n\n    #FULLY CONNECTED LAYER\n    layerList1 = [\n        FullyConnected(units=128, num_classes=dataset['num_classes'])\n    ]\n    layerList2 = []\n    daddy.block_list.append(Block(2, 5, layerList1, layerList2))\n    \n    \n\n    model = daddy.build_model()\n    print(\"Type of model_final:\", type(model))\n    daddy.train_and_evaluate(model, dataset)\n    return daddy","metadata":{"execution":{"iopub.status.busy":"2025-02-26T20:48:59.995719Z","iopub.execute_input":"2025-02-26T20:48:59.996412Z","iopub.status.idle":"2025-02-26T20:49:00.004672Z","shell.execute_reply.started":"2025-02-26T20:48:59.996380Z","shell.execute_reply":"2025-02-26T20:49:00.003764Z"},"trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# NETWORK\nimport tensorflow as tf\nimport os\nimport pickle\nfrom keras.callbacks import Callback\nfrom keras.models import Sequential\nfrom random import randint, choice\nfrom copy import deepcopy\n\n\nclass Network:\n    __slots__ = ('name', 'block_list', 'fitness', 'model')\n\n    def __init__(self, it):\n        self.name = 'parent_' + str(it) if it == 0 else 'net_' + str(it)\n        self.block_list = []\n        self.fitness = None\n        self.model = None\n\n    \"\"\"def build_model(self):\n        model = Sequential()                                # create model\n        for block in self.block_list:\n            for layer in block.get_layers():                # build model\n                try:\n                    layer.build_layer(model)\n                except:\n                    print(\"\\nINDIVIDUAL ABORTED, CREATING A NEW ONE\\n\")\n                    return -1\n        return model\"\"\"\n    def build_model(self):\n        model = Sequential()              \n        print(\"The block is:\")\n        print(self.block_list)                 # create model\n        for block in self.block_list:\n            #print(\"Building block type:\", block.type)\n            #print(\"TOTAL :::\")\n            #print(block.get_layer_name())\n            for layer in block.get_layers():                # build model\n                #print(\"Adding layer:\", layer.name)\n                try:\n                    layer.build_layer(model)\n                    print(\"Layer added successfully.\")\n                except Exception as e:\n                    print(\"Error occurred while adding layer:\", e)\n                    print(\"Returning None.\")\n                    return -1\n        print(\"Model successfully built.\")\n        return model\n\n    def train_and_evaluate(self, model, dataset):\n        print(\"Training\", self.name)\n        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n        try:\n            history = model.fit(dataset['x_train'],\n                                dataset['y_train'],\n                                batch_size=dataset['batch_size'],\n                                epochs=dataset['epochs'],\n                                validation_data=(dataset['x_val'], dataset['y_val']),\n                                shuffle=True)\n        except Exception as e:\n            print(\"An error occurred during model training:\", e)\n            return -1\n            # You can choose to handle the error in a specific way here, like logging it or taking corrective actions.\n\n\n        # Extract metrics from the training history\n        training_loss = history.history['loss'][-1]\n        training_accuracy = history.history['accuracy'][-1]\n        validation_loss = history.history['val_loss'][-1]\n        validation_accuracy = history.history['val_accuracy'][-1]\n\n        print(model.summary())\n        x= input(\"Do you want to continue ?\")\n        # Additional metrics (you can customize this based on your needs)\n        classification_error_rate = 1.0 - validation_accuracy\n\n        self.model = model  # Save the model\n        self.fitness = validation_loss  # Use validation loss as fitness\n\n        # Print metrics\n        print(\"SUMMARY OF\", self.name)\n        print(\"Training Loss:\", training_loss)\n        print(\"Training Accuracy:\", training_accuracy)\n        print(\"Validation Loss:\", validation_loss)\n        print(\"Validation Accuracy:\", validation_accuracy)\n        print(\"Classification Error Rate:\", classification_error_rate)\n\n        tf.keras.models.save_model(model, self.name + '.h5')         # save model\n        #model.save(self.name + '.h5')                       # save model\n        save_network(self)                                  # save topology, model and fitness\n\n    def asexual_reproduction(self, it, dataset):\n\n        # if the individual already exists, just load it\n        if os.path.isfile('net_' + str(it) + '.h5'):\n            print(\"\\n-------------------------------------\")\n            print(\"Loading individual net_\" + str(it))\n            print(\"--------------------------------------\\n\")\n            individual = load_network('net_' + str(it))\n            model = tf.keras.models.load_model(individual.name + '.h5')\n            print(\"SUMMARY OF\", individual.name)\n            print(model.summary())\n            print(\"FITNESS: \", individual.fitness)\n            return individual\n\n        # otherwise, create the individual by mutating the parent\n        individual = Network(it)\n\n        print(\"\\n-------------------------------------\")\n        print(\"\\nCreating individual\", individual.name)\n        print(\"--------------------------------------\\n\")\n\n        individual.block_list = deepcopy(self.block_list)           # copy the layer list from parent\n\n        print(\"----->Strong Mutation\")\n        individual.block_mutation(dataset)                          # mutate a block\n        individual.layer_mutation(dataset)                          # mutate a layer\n        individual.parameters_mutation()                            # mutate some parameters\n\n        model = individual.build_model()\n        \n        if model == -1:\n            return self.asexual_reproduction(it, dataset)\n        \n        if(individual.train_and_evaluate(model, dataset)==-1):\n            return self.asexual_reproduction(it, dataset)\n        else:\n            return individual\n            \n\n    def block_mutation(self, dataset):\n        try:\n            print(\"Block Mutation\")\n\n            print([(block.index, block.type) for block in self.block_list])\n\n            # block list containing all the blocks with type = 1\n            bl = [block.index for block in self.block_list if block.type == 1]\n\n            if len(bl) == 0:\n                print(\"Creating a new block with two Convolutional layers and a Pooling layer\")\n                self.block_list[1].index = 2\n                layerList1 = [\n                    Convolutional(filters=pow(2, randint(5, 8)),\n                                  filter_size=(3, 3),\n                                  stride_size=(1, 1),\n                                  padding='same',\n                                  input_shape=dataset['x_train'].shape[1:]),\n                    Convolutional(filters=pow(2, randint(5, 8)),\n                                  filter_size=(3, 3),\n                                  stride_size=(1, 1),\n                                  padding='same',\n                                  input_shape=dataset['x_train'].shape[1:])\n                ]\n                layerList2 = [\n                    Pooling(pool_size=(2, 2),\n                            stride_size=(2, 2),\n                            padding='same')\n                ]\n                b = Block(1, 1, layerList1, layerList2)\n                self.block_list.insert(1, b)\n                return\n\n            block_idx = randint(1, max(bl))         # pick a random block among all the blocks with type = 1\n            block_type_idx = randint(0, 1)          # 1 -> Conv2D; 0 -> Pooling or Dropout\n            mutation_type = randint(0, 1)           # 1 -> remove; 0 -> add\n\n            # list of layers of the selected block\n            layerList = self.block_list[block_idx].layerList1 if block_type_idx else self.block_list[block_idx].layerList2\n            length = len(layerList)\n\n            if mutation_type:                                       # remove\n                if length == 1:\n                    del self.block_list[block_idx]\n                elif block_type_idx:\n                    pos = randint(0, length - 1)\n                    print(\"Removing a Conv2D layer at\", pos)\n                    del layerList[pos]\n                else:\n                    pos = randint(0, length - 1)\n                    print(\"Removing a Pooling/Dropout layer at\", pos)\n                    del layerList[pos]\n            else:                                                   # add\n                if block_type_idx:\n                    print(\"Inserting a Convolutional layer\")\n                    layer = Convolutional(filters=pow(2, randint(5, 8)),\n                                          filter_size=(3, 3),\n                                          stride_size=(1, 1),\n                                          padding='same',\n                                          input_shape=dataset['x_train'].shape[1:])\n                    layerList.insert(randint(0, length - 1), layer)\n                else:\n                    if randint(0, 1):                               # 1 -> Pooling; 0 -> Dropout\n                        print(\"Inserting a Pooling layer\")\n                        layer = Pooling(pool_size=(2, 2),\n                                        stride_size=(2, 2),\n                                        padding='same')\n                        layerList.insert(randint(0, length - 1), layer)\n                    else:\n                        print(\"Inserting a Dropout layer\")\n                        rate = choice([0.15, 0.25, 0.35, 0.50])\n                        layer = Dropout(rate=rate)\n                        layerList.insert(randint(0, length - 1), layer)\n        except Exception as e:\n            print(f\"An error occurred during block mutation: {e}\")\n            return None\n\n                    \n                    \n                    \n                    \n                    \n\n    \"\"\"def layer_mutation(self, dataset):\n        print(\"Layer Mutation\")\n\n        # pick a random block among all the blocks with type = 1\n        bl = [block.index for block in self.block_list if block.type == 1]\n\n        if len(bl) == 0:\n            return\n\n        block_idx = randint(1, max(bl))\n        block_type_idx = randint(0, 1)      # 1 -> Conv2D; 0 -> Pooling or Dropout\n\n        # list of layers of the selected block\n        layerList = self.block_list[block_idx].layerList1 if block_type_idx else self.block_list[block_idx].layerList2\n\n        if len(layerList) == 0:\n            if block_type_idx:\n                layer = Convolutional(filters=pow(2, randint(5, 8)),\n                                      filter_size=(3, 3),\n                                      stride_size=(1, 1),\n                                      padding='same',\n                                      input_shape=dataset['x_train'].shape[1:])\n                self.block_list[block_idx].layerList1.append(layer)\n                return\n            else:\n                layer = Pooling(pool_size=(2, 2),\n                                stride_size=(2, 2),\n                                padding='same')\n                self.block_list[block_idx].layerList2.append(layer)\n\n        idx = randint(0, len(layerList) - 1)\n        layer = layerList[idx]\n\n        if layer.name == 'Conv2D':\n            print(\"Splitting Conv2D layer at index\", idx)\n            layer.filters = int(layer.filters * 0.5)\n            layerList.insert(idx, deepcopy(layer))\n        elif layer.name == 'MaxPooling2D' or layer.name == 'AveragePooling2D':\n            print(\"Changing Pooling layer at index\", idx, \"with Conv2D layer\")\n            del layerList[idx]\n            conv_layer = Convolutional(filters=pow(2, randint(5, 8)),\n                                       filter_size=(3, 3),\n                                       stride_size=(2, 2),\n                                       padding=layer.padding,\n                                       input_shape=dataset['x_train'].shape[1:])\n            layerList.insert(idx, conv_layer)\"\"\"\n    \n    def layer_mutation(self, dataset):\n        print(\"Layer Mutation\")\n\n        # Determine the maximum number of layers that can be added or removed\n        max_layers_to_add = 16 - sum(len(block.layerList1) + len(block.layerList2) for block in self.block_list)\n        max_layers_to_remove = sum(len(block.layerList1) + len(block.layerList2) - 1 for block in self.block_list)\n\n        if max_layers_to_add == 0 and max_layers_to_remove == 0:\n            return\n\n        # Pick a random block among all the blocks with type = 1\n        bl = [block.index for block in self.block_list if block.type == 1]\n\n        if len(bl) == 0:\n            return\n\n        block_idx = randint(1, max(bl))\n        block_type_idx = randint(0, 1)      # 1 -> Conv2D; 0 -> Pooling or Dropout\n\n        # List of layers of the selected block\n        layerList = self.block_list[block_idx].layerList1 if block_type_idx else self.block_list[block_idx].layerList2\n\n        if len(layerList) == 0:\n            if block_type_idx:\n                layer = Convolutional(filters=pow(2, randint(5, 8)),\n                                      filter_size=(3, 3),\n                                      stride_size=(1, 1),\n                                      padding='same',\n                                      input_shape=dataset['x_train'].shape[1:])\n                self.block_list[block_idx].layerList1.append(layer)\n            else:\n                layer = Pooling(pool_size=(2, 2),\n                                stride_size=(2, 2),\n                                padding='same')\n                self.block_list[block_idx].layerList2.append(layer)\n        else:\n            # Randomly choose whether to add or remove a layer\n            add_layer = bool(randint(0, 1))\n\n            if add_layer and max_layers_to_add > 0:\n                # Add a layer\n                layer = self.create_random_layer(dataset)\n                layerList.insert(randint(0, len(layerList)), layer)\n            elif not add_layer and max_layers_to_remove > 0:\n                # Remove a layer\n                idx = randint(0, len(layerList) - 1)\n                del layerList[idx]\n\n        # Ensure the total number of layers in the block doesn't exceed 16\n        if len(self.block_list[block_idx].layerList1) + len(self.block_list[block_idx].layerList2) > 16:\n            # Remove a random layer to maintain the total count of 16 layers\n            block_layerList = self.block_list[block_idx].layerList1 if block_type_idx else self.block_list[block_idx].layerList2\n            del block_layerList[randint(0, len(block_layerList) - 1)]\n\n    def create_random_layer(self, dataset):\n        # Create a random layer (Conv2D or Pooling)\n        if randint(0, 1):\n            # Conv2D layer\n            return Convolutional(filters=pow(2, randint(5, 8)),\n                                 filter_size=(3, 3),\n                                 stride_size=(1, 1),\n                                 padding='same',\n                                 input_shape=dataset['x_train'].shape[1:])\n        else:\n            # Pooling layer\n            return Pooling(pool_size=(2, 2),\n                           stride_size=(2, 2),\n                           padding='same')\n\n            \n            \n            \n            \n            \n            \n            \n            \n\n    def parameters_mutation(self):\n        print(\"Parameters Mutation\")\n        for block in self.block_list:\n            for layer in block.get_layers():\n                if randint(0, 1):\n                    layer.mutate_parameters()\n\n    def save_network_info(self, info_filename):\n        network_info = {\n            'name': self.name,\n            'block_list': self.block_list,\n            'fitness': self.fitness\n        }\n\n        with open(info_filename, 'wb') as info_file:\n            pickle.dump(network_info, info_file)\n\n    def load_network_info(self, info_filename):\n        with open(info_filename, 'rb') as info_file:\n            network_info = pickle.load(info_file)\n\n        self.name = network_info['name']\n        self.block_list = network_info['block_list']\n        self.fitness = network_info['fitness']\n\n    def save_model(self, model_filename):\n        self.model.save(model_filename)\n\n    def load_model(self, model_filename):\n        self.model = tf.keras.models.load_model(model_filename)\n\n    def save_network(self, network_info_filename, model_filename):\n        # Save non-model attributes\n        self.save_network_info(network_info_filename)\n\n        # Save the model separately\n        self.save_model(model_filename)\n\n    def load_network(self, network_info_filename, model_filename):\n        # Load non-model attributes\n        self.load_network_info(network_info_filename)\n\n        # Load the model separately\n        self.load_model(model_filename)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-26T20:49:11.516938Z","iopub.execute_input":"2025-02-26T20:49:11.517232Z","iopub.status.idle":"2025-02-26T20:49:11.553941Z","shell.execute_reply.started":"2025-02-26T20:49:11.517211Z","shell.execute_reply":"2025-02-26T20:49:11.552970Z"},"trusted":true},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# TOPOLOGY\n\nimport keras.layers\nfrom random import randint\n\n\nclass Block:\n\t__slots__ = ('type', 'index', 'layerList1', 'layerList2')\n\n\tdef __init__(self, type, index, layerList1, layerList2):\n\t\tself.type = type\t\t\t\t\t\t\t\t\t\t# 0 -> initial layer; 1 -> mid layers; 2 -> final layer\n\t\tself.index = index\t\t\t\t\t\t\t\t\t\t# block index among all the blocks\n\t\tself.layerList1 = layerList1\t\t\t\t\t\t\t# Convolutional layers\n\t\tself.layerList2 = layerList2\t\t\t\t\t\t\t# Pooling and Dropout layers\n\n\tdef get_layers(self):\n\t\treturn self.layerList1 + self.layerList2\n\n\tdef get_size(self):\n\t\treturn len(self.get_layers())\n\n\nclass Convolutional:\n\t# __slots__ = ('name', 'filters', 'padding', 'filter_size', 'stride_size', 'input_shape')\n\n\tdef __init__(self, filters, padding, filter_size, stride_size, input_shape):\n\t\tself.name = 'Conv2D'\n\t\tself.filters = filters\n\t\tself.padding = padding\n\t\tself.filter_size = filter_size\n\t\tself.stride_size = stride_size\n\t\tself.input_shape = input_shape\n\n\tdef build_layer(self, model):\n\t\ttry:\n\t\t\tmodel.add(keras.layers.Conv2D(filters=self.filters,\n\t\t\t\t\t\t\t\t\t\t\tkernel_size=self.filter_size,\n\t\t\t\t\t\t\t\t\t\t\tstrides=self.stride_size,\n\t\t\t\t\t\t\t\t\t\t\tpadding=self.padding,\n\t\t\t\t\t\t\t\t\t\t\tactivation='relu',\n\t\t\t\t\t\t\t\t\t\t\tkernel_initializer='he_uniform',\n\t\t\t\t\t\t\t\t\t\t\tinput_shape=self.input_shape))\n\t\texcept ValueError as e:\n\t\t\tprint(\"Error occurred while adding layer:\", e)\n\t\t\tprint(\"Skipping current architecture.\")\n\t\t\treturn  # Skip adding this layer\n\tdef mutate_parameters(self):\n\t\tmutation = randint(0, 2)  # Adjusted the number of mutations\n\t\tprint(\"Mutating\", self.name, \"layer:\")\n\t\tif mutation == 0 and self.filters >= 64:  # Adjusted the filter reduction threshold\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters = int(self.filters / 2)\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 1 and self.filters <= 256:  # Adjusted the filter increase threshold\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters *= 2\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 2:\n\t\t\tif self.padding == 'valid':\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'same'\n\t\t\t\tprint(\"to \", self.padding)\n\t\t\telse:\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'valid'\n\t\t\t\tprint(\"to \", self.padding)\n\n        \n\n        \n\n\n\t\"\"\"def mutate_parameters(self):\n\t\tmutation = randint(0, 4)\n\t\tprint(\"Mutating\", self.name, \"layer:\")\n\t\tif mutation == 0 and self.filters >= 32:\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters = int(self.filters / 2)\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 1 and self.filters >= 32:\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters = int(self.filters / 2)\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 2 and self.filters <= 512:\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters *= 2\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 3 and self.filters <= 512:\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters *= 2\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 4:\n\t\t\tif self.padding == 'valid':\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'same'\n\t\t\t\tprint(\"to \", self.padding)\n\t\t\telse:\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'valid'\n\t\t\t\tprint(\"to \", self.padding)\n\"\"\"\n    \n\n'''\nelif mutation is 4:\n\tprint(\"changed self.stride_size from \", self.stride_size, \" \", end=\"\")\n\tself.stride_size = (self.stride_size[0] + 1, self.stride_size[1] + 1)\n\tprint(\"to \", self.stride_size, \" and \", end=\"\")\n\tprint(\"changed self.stride_size from \", self.stride_size, \" \", end=\"\")\n\tself.stride_size = (self.stride_size[0] + 1, self.stride_size[1] + 1)\n\tprint(\"to \", self.stride_size)\n'''\n\n\nclass Pooling:\n\t__slots__ = ('name', 'pool_size', 'stride_size', 'padding')\n\n\tdef __init__(self, pool_size, stride_size, padding):\n\t\tself.name = 'MaxPooling2D'\n\t\tself.pool_size = pool_size\n\t\tself.stride_size = stride_size\n\t\tself.padding = padding\n\n\tdef build_layer(self, model):\n\t\tif self.name == 'MaxPooling2D':\n\t\t\tmodel.add(keras.layers.MaxPooling2D(self.pool_size, self.stride_size, self.padding))\n\t\telif self.name == 'AveragePooling2D':\n\t\t\tmodel.add(keras.layers.AveragePooling2D(self.pool_size, self.stride_size, self.padding))\n\n\tdef mutate_parameters(self):\n\t\tprint(\"Mutating\", self.name, \"layer:\")\n\t\tmutation = randint(0, 1)\n\t\tif mutation == 0:\n\t\t\tif self.padding == 'valid':\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'same'\n\t\t\t\tprint(\"to \", self.padding)\n\t\t\telse:\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'valid'\n\t\t\t\tprint(\"to \", self.padding)\n\t\telif mutation == 1:\n\t\t\tif self.name == 'MaxPooling2D':\n\t\t\t\tprint(\"-->changed self.name from \", self.name, \" \", end=\"\")\n\t\t\t\tself.name = 'AveragePooling2D'\n\t\t\t\tprint(\"to \", self.name)\n\t\t\telse:\n\t\t\t\tprint(\"-->changed self.name from \", self.name, \" \", end=\"\")\n\t\t\t\tself.name = 'MaxPooling2D'\n\t\t\t\tprint(\"to \", self.name)\n\n\n'''\nif mutation is 0:\n\tprint(\"changed self.stride_size from \", self.stride_size, \" \", end=\"\")\n\tself.stride_size = (self.stride_size[0] + 1, self.stride_size[1] + 1)\n\tprint(\"to \", self.stride_size)\n'''\n\n\nclass FullyConnected:\n\t__slots__ = ('name', 'units', 'num_classes')\n\n\tdef __init__(self, units, num_classes):\n\t\tself.name = \"FullyConnected\"\n\t\tself.units = units\n\t\tself.num_classes = num_classes\n\n\tdef build_layer(self, model):\n\t\tmodel.add(keras.layers.Flatten())\n\t\tmodel.add(keras.layers.Dense(self.units, activation='relu', kernel_initializer='he_uniform'))\n\t\tmodel.add(keras.layers.Dense(self.num_classes, activation='softmax'))\n\n\tdef mutate_parameters(self):\n\t\tprint(\"Mutating\", self.name, \"layer:\")\n\t\tmutation = randint(0, 2)\n\t\tif mutation == 0:\n\t\t\tprint(\"-->changed self.units from \", self.units, \" \", end=\"\")\n\t\t\tself.units *= 2\n\t\t\tprint(\"to \", self.units)\n\t\telif mutation == 1:\n\t\t\tprint(\"-->changed self.units from \", self.units, \" \", end=\"\")\n\t\t\tself.units *= 2\n\t\t\tprint(\"to \", self.units)\n\t\telif mutation == 2:\n\t\t\tprint(\"-->changed self.units from \", self.units, \" \", end=\"\")\n\t\t\tself.units /= 2\n\t\t\tprint(\"to \", self.units)\n\n\n'''\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(self.num_classes, activation='softmax'))\n'''\n\n\nclass Dropout:\n\t__slots__ = ('name', 'rate')\n\n\tdef __init__(self, rate):\n\t\tself.name = \"Dropout\"\n\t\tself.rate = rate\n\n\tdef build_layer(self, model):\n\t\tmodel.add(keras.layers.Dropout(self.rate))\n\n\tdef mutate_parameters(self):\n\t\tprint(\"Mutating\", self.name, \"layer:\")\n\t\tmutation = randint(0, 3)\n\t\tif mutation == 0 and self.rate <= 0.85:\n\t\t\tprint(\"-->changed self.rate from \", self.rate, \" \", end=\"\")\n\t\t\tself.rate = self.rate + 0.10\n\t\t\tprint(\"to \", self.rate)\n\t\telif mutation == 1 and self.rate <= 0.90:\n\t\t\tprint(\"-->changed self.rate from \", self.rate, \" \", end=\"\")\n\t\t\tself.rate = self.rate + 0.05\n\t\t\tprint(\"to \", self.rate)\n\t\telif mutation == 2 and self.rate >= 0.15:\n\t\t\tprint(\"-->changed self.rate from \", self.rate, \" \", end=\"\")\n\t\t\tself.rate = self.rate - 0.10\n\t\t\tprint(\"to \", self.rate)\n\t\telif mutation == 3 and self.rate >= 0.10:\n\t\t\tprint(\"-->changed self.rate from \", self.rate, \" \", end=\"\")\n\t\t\tself.rate = self.rate - 0.05\n\t\t\tprint(\"to \", self.rate)\n\nclass FlattenLayer:\n    def __init__(self):\n        self.name = 'Flatten'\n\n    def build_layer(self, model):\n        model.add(keras.layers.Flatten())\n\n    def mutate_parameters(self):\n        # The Flatten layer does not have any parameters to mutate\n        pass\n","metadata":{"execution":{"iopub.status.busy":"2025-02-26T20:49:25.000530Z","iopub.execute_input":"2025-02-26T20:49:25.000932Z","iopub.status.idle":"2025-02-26T20:49:25.023446Z","shell.execute_reply.started":"2025-02-26T20:49:25.000904Z","shell.execute_reply":"2025-02-26T20:49:25.022553Z"},"trusted":true},"outputs":[],"execution_count":49},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport os\nfrom copy import deepcopy\nfrom random import sample\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)      # suppress messages from Tensorflow\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n\ndef initialize_population(population_size, dataset):\n    print(\"----->Initializing Population\")\n    daddy = compute_parent(dataset)                                 # load parent from input\n    population = [daddy]\n    for it in range(1, population_size):\n        population.append(daddy.asexual_reproduction(it, dataset))\n\n    # sort population on ascending order based on fitness\n    return sorted(population, key=lambda cnn: cnn.fitness)\n\n\ndef selection(k, population, num_population):\n    if k == 0:                                              # elitism selection\n        print(\"----->Elitism selection\")\n        return population[0], population[1]\n    elif k == 1:                                            # tournament selection\n        print(\"----->Tournament selection\")\n        i = randint(0, num_population - 1)\n        j = i\n        while j < num_population - 1:\n            j += 1\n            if randint(1, 100) <= 50:\n                return population[i], population[j]\n        return population[i], population[0]\n    else:                                                   # proportionate selection\n        print(\"----->Proportionate selection\")\n        cum_sum = 0\n        for i in range(num_population):\n            cum_sum += population[i].fitness\n        perc_range = []\n        for i in range(num_population):\n            count = 100 - int(100 * population[i].fitness / cum_sum)\n            for j in range(count):\n                perc_range.append(i)\n        i, j = sample(range(1, len(perc_range)), 2)\n        while i == j:\n            i, j = sample(range(1, len(perc_range)), 2)\n        return population[perc_range[i]], population[perc_range[j]]\n\n\ndef crossover(parent1, parent2, it):\n    print(\"----->Crossover\")\n    child = Network(it)\n\n    first, second = None, None\n    if randint(0, 1):\n        first = parent1\n        second = parent2\n    else:\n        first = parent2\n        second = parent1\n\n    child.block_list = deepcopy(first.block_list[:randint(1, len(first.block_list) - 1)]) \\\n                       + deepcopy(second.block_list[randint(1, len(second.block_list) - 1):])\n\n    order_indexes(child)                            # order the indexes of the blocks\n\n    return child\n\n\ndef genetic_algorithm(num_population, num_generation, num_offspring, dataset, early_stopping_generations=3):\n    print(\"Genetic Algorithm\")\n\n    population = initialize_population(num_population, dataset)\n\n    print(\"\\n-------------------------------------\")\n    print(\"Initial Population:\")\n    for cnn in population:\n        print(cnn.name, ': ', cnn.fitness)\n    print(\"--------------------------------------\\n\")\n\n    # for printing statistics about fitness and the number of parameters of the best individual\n    stats = [(population[0].fitness, population[0].model.count_params())]\n\n    # Initialize a variable to keep track of consecutive generations with the same best fitness\n    consecutive_same_fitness = 0\n\n    for gen in range(1, num_generation + 1):\n        '''\n            k is the selection parameter:\n                k = 0 -> elitism selection\n                k = 1 -> tournament selection\n                k = 2 -> proportionate selection\n        '''\n        k = randint(0, 2)\n\n        print(\"\\n------------------------------------\")\n        print(\"Generation -----------------------------------------------------------------------------------\", gen)\n        print(\"-------------------------------------\")\n\n        for c in range(num_offspring):\n\n            print(\"\\nCreating Child\", c)\n\n            parent1, parent2 = selection(k, population, num_population)                 # selection\n            print(\"Selected\", parent1.name, \"and\", parent2.name, \"for reproduction\")\n\n            child = crossover(parent1, parent2, c + num_population)                     # crossover\n            print(\"Child has been created\")\n\n            print(\"----->Soft Mutation\")\n            child.layer_mutation(dataset)                                               # mutation\n            child.parameters_mutation()\n            print(\"Child has been mutated\")\n\n            model = child.build_model()                                                 # evaluation\n\n            while model == -1:\n                child = crossover(parent1, parent2, c + num_population)\n                child.block_mutation(dataset)\n                child.layer_mutation(dataset)\n                child.parameters_mutation()\n                model = child.build_model()\n\n            child.train_and_evaluate(model, dataset)\n\n            if child.fitness < population[-1].fitness:                                  # evolve population\n                print(\"----->Evolution: Child\", child.name, \"with fitness\", child.fitness, \"replaces parent \", end=\"\")\n                print(population[-1].name, \"with fitness\", population[-1].fitness)\n                name = population[-1].name\n\n                child.save_network(\"child_model_info.pkl\", \"child_model.h5\")\n                population[-1].load_network(\"child_model_info.pkl\", \"child_model.h5\")\n\n                population[-1].name = name\n                population = sorted(population, key=lambda net: net.fitness)\n            else:\n                print(\"----->Evolution: Child\", child.name, \"with fitness\", child.fitness, \"is discarded\")\n        \n        if gen >= 3 and all(population[i].fitness == population[i + 1].fitness for i in range(-3, -1)):\n            consecutive_same_fitness += 1\n            print(f\"Consecutive generations with the same best fitness: {consecutive_same_fitness}\")\n        if consecutive_same_fitness >= 3:\n            print(\"Stopping the algorithm as the best fitness has remained the same for the last 3 generations.\")\n            break\n    else:\n        consecutive_same_fitness = 0\n        \n       #Check if the best fitness has remained the same for the last early_stopping_generations generations\n        if all(population[i].fitness == population[i + 1].fitness for i in range(-early_stopping_generations, -1)):\n            consecutive_same_fitness += 1\n            print(f\"Consecutive generations with the same best fitness: {consecutive_same_fitness}\")\n            if consecutive_same_fitness == early_stopping_generations:\n                print(f\"Stopping the algorithm as the best fitness has remained the same for {early_stopping_generations} generations.\")\n        else:\n            consecutive_same_fitness = 0\n        stats.append((population[0].fitness, population[0].model.count_params()))\n\n    print(\"\\n\\n-------------------------------------\")\n    print(\"Final Population\")\n    print(\"-------------------------------------\\n\")\n    for cnn in population:\n        print(cnn.name, ': ', cnn.fitness)\n\n    print(\"\\n-------------------------------------\")\n    print(\"Stats\")\n    for i in range(len(stats)):\n        print(\"Best individual at generation\", i + 1, \"has fitness\", stats[i][0], \"and parameters\", stats[i][1])\n    print(\"-------------------------------------\\n\")\n\n    # plot the fitness and the number of parameters of the best individual at each iteration\n    plot_statistics(stats)\n\n    return population[0]\n\n\n\ndef main(): \n        \n        #with strategy.scope():\n        #from tensorflow.python.client import device_lib\n        #print(device_lib.list_local_devices())\n        #batch_size = 8\n        #batch_size = batch_size * strategy.num_replicas_in_sync\n        batch_size = 32                       # the number of training examples in one forward/backward pass\n        num_classes = 10                        # number of cifar-10 dataset classes\n        epochs =20              # number of forward and backward passes of all the training examples\n\n        '''\n            dataset contains the hyper parameters for loading data and the dataset:\n                dataset = {\n                    'batch_size': batch_size,\n                    'num_classes': num_classes,\n                    'epochs': epochs,\n                    'x_train': x_train,\n                    'x_test': x_test,\n                    'y_train': y_train,\n                    'y_test': y_test\n                }\n        '''\n        from tensorflow.keras.datasets import cifar10\n\n        (x_train, y_train), (x_test, y_test)=cifar10.load_data()\n        \n        print('Shape of x_train is {}'.format(x_train.shape))\n        print('Shape of x_test is {}'.format(x_test.shape)) \n        print('Shape of y_train is {}'.format(y_train.shape))\n        print('Shape of y_test is {}'.format(y_test.shape))\n\n        from tensorflow.keras.utils import to_categorical\n\n        # Normalizing\n        x_train=x_train/255\n        x_test=x_test/255\n        \n        #One hot encoding\n        y_train_cat=to_categorical(y_train,10)\n        y_test_cat=to_categorical(y_test,10)\n\n    \n        dataset = {\n        'batch_size': batch_size,\n        'num_classes': num_classes,\n        'epochs': epochs,\n        'x_train': x_train,\n        'y_train': y_train_cat,\n        'x_val': x_test,\n        'y_val': y_test_cat\n    }\n\n\n        \"\"\"model.fit(dataset['x_train'],\n                                dataset['y_train'],\n                                batch_size=dataset['batch_size'],\n                                epochs=dataset['epochs'],\n                                validation_data=(dataset['x_val'], dataset['y_val']),\n                                shuffle=True)\n                                \n         history1=model1.fit(x_train,y_train_cat,epochs=20,validation_data=(x_test,y_test_cat))   \n                                \"\"\"\n\n\n\n    \n        #dataset = load_dataset(batch_size, num_classes, epochs)\n\n        num_population = 10\n        num_generation = 10\n        num_offspring = 4\n\n        # plot the best model obtained\n        optCNN = genetic_algorithm(num_population, num_generation, num_offspring, dataset)\n\n        # plot the training and validation loss and accuracy\n        num_epoch = 20\n        model = optCNN.build_model()\n        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n        history = model.fit(dataset['x_train'],\n                            dataset['y_train'],\n                            batch_size=dataset['batch_size'],\n                            epochs=num_epoch,\n                            validation_data=(dataset['x_test'], dataset['y_test']),\n                            shuffle=True)\n        optCNN.model = model                                        # model\n        optCNN.fitness = history.history['val_loss'][-1]            # fitness\n\n        print(\"\\n\\n-------------------------------------\")\n        print(\"The Final CNN has been evolved successfully in the individual\", optCNN.name)\n        print(\"-------------------------------------\\n\")\n        daddy = load_network('parent_0')\n        model = tf.keras.models.load_model('parent_0.h5')\n        print(\"\\n\\n-------------------------------------\")\n        print(\"Summary of initial CNN\")\n        print(model.summary())\n        print(\"Fitness of initial CNN:\", daddy.fitness)\n\n        print(\"\\n\\n-------------------------------------\")\n        print(\"Summary of evolved individual\")\n        print(optCNN.model.summary())\n        print(\"Fitness of the evolved individual:\", optCNN.fitness)\n        print(\"-------------------------------------\\n\")\n\n        plot_training(history)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2025-02-26T20:49:28.935677Z","iopub.execute_input":"2025-02-26T20:49:28.936007Z","iopub.status.idle":"2025-02-26T20:51:48.041645Z","shell.execute_reply.started":"2025-02-26T20:49:28.935981Z","shell.execute_reply":"2025-02-26T20:51:48.040373Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Shape of x_train is (50000, 32, 32, 3)\nShape of x_test is (10000, 32, 32, 3)\nShape of y_train is (50000, 1)\nShape of y_test is (10000, 1)\nGenetic Algorithm\n----->Initializing Population\nThe block is:\n[<__main__.Block object at 0x7ac3b7556d80>, <__main__.Block object at 0x7ac3977e4a80>, <__main__.Block object at 0x7ac3973cc2c0>]\nLayer added successfully.\nLayer added successfully.\nLayer added successfully.\nLayer added successfully.\nLayer added successfully.\nLayer added successfully.\nLayer added successfully.\nModel successfully built.\nType of model_final: <class 'keras.src.engine.sequential.Sequential'>\nTraining parent_0\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"2025-02-26 20:49:32.926894: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_10/dropout_15/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"1563/1563 [==============================] - 8s 4ms/step - loss: 1.4557 - accuracy: 0.4823 - val_loss: 1.4302 - val_accuracy: 0.5009\nEpoch 2/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.1594 - accuracy: 0.5956 - val_loss: 1.3921 - val_accuracy: 0.5124\nEpoch 3/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.0609 - accuracy: 0.6321 - val_loss: 1.4985 - val_accuracy: 0.5279\nEpoch 4/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.0072 - accuracy: 0.6560 - val_loss: 1.3451 - val_accuracy: 0.5617\nEpoch 5/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9672 - accuracy: 0.6718 - val_loss: 1.6466 - val_accuracy: 0.5202\nEpoch 6/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9502 - accuracy: 0.6827 - val_loss: 1.6873 - val_accuracy: 0.5443\nEpoch 7/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9470 - accuracy: 0.6854 - val_loss: 1.8869 - val_accuracy: 0.5507\nEpoch 8/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9471 - accuracy: 0.6925 - val_loss: 1.5875 - val_accuracy: 0.6045\nEpoch 9/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9507 - accuracy: 0.6916 - val_loss: 1.9015 - val_accuracy: 0.5837\nEpoch 10/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9780 - accuracy: 0.6914 - val_loss: 1.6233 - val_accuracy: 0.5971\nEpoch 11/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9844 - accuracy: 0.6883 - val_loss: 1.6379 - val_accuracy: 0.6204\nEpoch 12/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.0175 - accuracy: 0.6843 - val_loss: 1.4064 - val_accuracy: 0.6426\nEpoch 13/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.0702 - accuracy: 0.6738 - val_loss: 2.1687 - val_accuracy: 0.5928\nEpoch 14/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.0825 - accuracy: 0.6753 - val_loss: 1.9309 - val_accuracy: 0.5808\nEpoch 15/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.1252 - accuracy: 0.6617 - val_loss: 1.2343 - val_accuracy: 0.6476\nEpoch 16/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.2006 - accuracy: 0.6441 - val_loss: 1.3499 - val_accuracy: 0.5862\nEpoch 17/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.2291 - accuracy: 0.6319 - val_loss: 2.4157 - val_accuracy: 0.5033\nEpoch 18/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.3137 - accuracy: 0.6079 - val_loss: 1.7378 - val_accuracy: 0.6080\nEpoch 19/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.3612 - accuracy: 0.5959 - val_loss: 1.3490 - val_accuracy: 0.5722\nEpoch 20/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.3720 - accuracy: 0.5782 - val_loss: 1.2069 - val_accuracy: 0.6064\nModel: \"sequential_10\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n conv2d_18 (Conv2D)          (None, 32, 32, 32)        896       \n                                                                 \n max_pooling2d_15 (MaxPooli  (None, 16, 16, 32)        0         \n ng2D)                                                           \n                                                                 \n dropout_15 (Dropout)        (None, 16, 16, 32)        0         \n                                                                 \n conv2d_19 (Conv2D)          (None, 16, 16, 32)        9248      \n                                                                 \n max_pooling2d_16 (MaxPooli  (None, 8, 8, 32)          0         \n ng2D)                                                           \n                                                                 \n dropout_16 (Dropout)        (None, 8, 8, 32)          0         \n                                                                 \n flatten_7 (Flatten)         (None, 2048)              0         \n                                                                 \n dense_14 (Dense)            (None, 128)               262272    \n                                                                 \n dense_15 (Dense)            (None, 10)                1290      \n                                                                 \n=================================================================\nTotal params: 273706 (1.04 MB)\nTrainable params: 273706 (1.04 MB)\nNon-trainable params: 0 (0.00 Byte)\n_________________________________________________________________\nNone\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[50], line 278\u001b[0m\n\u001b[1;32m    274\u001b[0m         plot_training(history)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[50], line 243\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    240\u001b[0m num_offspring \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# plot the best model obtained\u001b[39;00m\n\u001b[0;32m--> 243\u001b[0m optCNN \u001b[38;5;241m=\u001b[39m \u001b[43mgenetic_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_population\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_offspring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# plot the training and validation loss and accuracy\u001b[39;00m\n\u001b[1;32m    246\u001b[0m num_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n","Cell \u001b[0;32mIn[50], line 73\u001b[0m, in \u001b[0;36mgenetic_algorithm\u001b[0;34m(num_population, num_generation, num_offspring, dataset, early_stopping_generations)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenetic_algorithm\u001b[39m(num_population, num_generation, num_offspring, dataset, early_stopping_generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenetic Algorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m     population \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_population\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial Population:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[50], line 12\u001b[0m, in \u001b[0;36minitialize_population\u001b[0;34m(population_size, dataset)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_population\u001b[39m(population_size, dataset):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----->Initializing Population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     daddy \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m                                 \u001b[38;5;66;03m# load parent from input\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     population \u001b[38;5;241m=\u001b[39m [daddy]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, population_size):\n","Cell \u001b[0;32mIn[47], line 52\u001b[0m, in \u001b[0;36mcompute_parent\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     50\u001b[0m model \u001b[38;5;241m=\u001b[39m daddy\u001b[38;5;241m.\u001b[39mbuild_model()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of model_final:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(model))\n\u001b[0;32m---> 52\u001b[0m \u001b[43mdaddy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m daddy\n","Cell \u001b[0;32mIn[48], line 73\u001b[0m, in \u001b[0;36mNetwork.train_and_evaluate\u001b[0;34m(self, model, dataset)\u001b[0m\n\u001b[1;32m     70\u001b[0m validation_accuracy \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39msummary())\n\u001b[0;32m---> 73\u001b[0m x\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDo you want to continue ?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Additional metrics (you can customize this based on your needs)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m classification_error_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m validation_accuracy\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"],"ename":"KeyboardInterrupt","evalue":"Interrupted by user","output_type":"error"}],"execution_count":50},{"cell_type":"markdown","source":"## To remove a folder\n# Clear output folder\nimport os\n\ndef remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working'\nremove_folder_contents(folder_path)\nos.rmdir(folder_path)","metadata":{"execution":{"iopub.status.busy":"2025-02-26T20:48:26.919941Z","iopub.execute_input":"2025-02-26T20:48:26.920268Z","iopub.status.idle":"2025-02-26T20:48:26.949259Z","shell.execute_reply.started":"2025-02-26T20:48:26.920243Z","shell.execute_reply":"2025-02-26T20:48:26.948047Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:15:11.525935Z","iopub.execute_input":"2025-02-26T20:15:11.526607Z","iopub.status.idle":"2025-02-26T20:15:11.551649Z","shell.execute_reply.started":"2025-02-26T20:15:11.526578Z","shell.execute_reply":"2025-02-26T20:15:11.550511Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msummary()\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32                       # the number of training examples in one forward/backward pass\nnum_classes = 10                        # number of cifar-10 dataset classes\nepochs =20   \ndataset = load_dataset(batch_size, num_classes, epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:18:14.960765Z","iopub.execute_input":"2025-02-26T20:18:14.961398Z","iopub.status.idle":"2025-02-26T20:18:16.106851Z","shell.execute_reply.started":"2025-02-26T20:18:14.961369Z","shell.execute_reply":"2025-02-26T20:18:16.106085Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Importing the necessary libraries, which we may or may not use. Its always good idea to import them befor (if you remember) else we can do it at any point of time no problem.\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten,Dropout,Input, AveragePooling2D, Activation,Conv2D, MaxPooling2D, BatchNormalization,Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard\nfrom tensorflow.keras import regularizers, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:29:56.995452Z","iopub.execute_input":"2025-02-26T20:29:56.996021Z","iopub.status.idle":"2025-02-26T20:29:57.003064Z","shell.execute_reply.started":"2025-02-26T20:29:56.995990Z","shell.execute_reply":"2025-02-26T20:29:57.002444Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\n\n(x_train, y_train), (x_test, y_test)=cifar10.load_data()\n\nprint('Shape of x_train is {}'.format(x_train.shape))\nprint('Shape of x_test is {}'.format(x_test.shape)) \nprint('Shape of y_train is {}'.format(y_train_cat.shape))\nprint('Shape of y_test is {}'.format(y_test_cat.shape))\n\nfrom tensorflow.keras.utils import to_categorical\n\n# Normalizing\nx_train=x_train/255\nx_test=x_test/255\n\n#One hot encoding\ny_train_cat=to_categorical(y_train,10)\ny_test_cat=to_categorical(y_test,10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:41:57.635269Z","iopub.execute_input":"2025-02-26T20:41:57.635633Z","iopub.status.idle":"2025-02-26T20:41:59.000749Z","shell.execute_reply.started":"2025-02-26T20:41:57.635605Z","shell.execute_reply":"2025-02-26T20:41:58.999749Z"}},"outputs":[{"name":"stdout","text":"Shape of x_train is (50000, 32, 32, 3)\nShape of x_test is (10000, 32, 32, 3)\nShape of y_train is (50000, 10)\nShape of y_test is (10000, 10, 10)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"print('Shape of x_train is {}'.format(dataset['x_train'].shape))\nprint('Shape of x_test is {}'.format(dataset['x_test'].shape)) \nprint('Shape of y_train is {}'.format(dataset['y_train'].shape))\nprint('Shape of y_test is {}'.format(dataset['y_test'].shape))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:36:20.955410Z","iopub.execute_input":"2025-02-26T20:36:20.956260Z","iopub.status.idle":"2025-02-26T20:36:20.961334Z","shell.execute_reply.started":"2025-02-26T20:36:20.956228Z","shell.execute_reply":"2025-02-26T20:36:20.960433Z"}},"outputs":[{"name":"stdout","text":"Shape of x_train is (50000, 32, 32, 3)\nShape of x_test is (10000, 32, 32, 3)\nShape of y_train is (50000, 10)\nShape of y_test is (10000, 10)\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"model1=Sequential()\nmodel1.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(Flatten())\nmodel1.add(Dense(256,activation='relu'))\nmodel1.add(Dense(10,activation='softmax'))\nmodel1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n# training_steps = train_generator.samples//train_generator.batch_size\n# validation_steps=test_generator.samples//test_generator.batch_size\n# history=model1.fit_generator(train_generator,epochs=50,steps_per_epoch=training_steps,validation_data=test_generator,validation_steps=validation_steps,callbacks=[board])\nhistory1=model1.fit(x_train,y_train_cat,epochs=20,validation_data=(x_test,y_test_cat))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:33:27.315261Z","iopub.execute_input":"2025-02-26T20:33:27.316173Z","iopub.status.idle":"2025-02-26T20:35:29.765371Z","shell.execute_reply.started":"2025-02-26T20:33:27.316142Z","shell.execute_reply":"2025-02-26T20:35:29.764352Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n1563/1563 [==============================] - 8s 4ms/step - loss: 1.5148 - accuracy: 0.4520 - val_loss: 1.3265 - val_accuracy: 0.5361\nEpoch 2/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.1916 - accuracy: 0.5775 - val_loss: 1.0876 - val_accuracy: 0.6223\nEpoch 3/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.0279 - accuracy: 0.6405 - val_loss: 1.0639 - val_accuracy: 0.6334\nEpoch 4/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9205 - accuracy: 0.6792 - val_loss: 1.0122 - val_accuracy: 0.6513\nEpoch 5/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.8350 - accuracy: 0.7075 - val_loss: 0.9852 - val_accuracy: 0.6588\nEpoch 6/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.7655 - accuracy: 0.7327 - val_loss: 1.0358 - val_accuracy: 0.6510\nEpoch 7/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6995 - accuracy: 0.7551 - val_loss: 0.9811 - val_accuracy: 0.6740\nEpoch 8/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6356 - accuracy: 0.7793 - val_loss: 0.9821 - val_accuracy: 0.6823\nEpoch 9/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.5789 - accuracy: 0.7980 - val_loss: 1.0285 - val_accuracy: 0.6783\nEpoch 10/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.5223 - accuracy: 0.8156 - val_loss: 1.0579 - val_accuracy: 0.6810\nEpoch 11/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.4679 - accuracy: 0.8339 - val_loss: 1.1646 - val_accuracy: 0.6650\nEpoch 12/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.4226 - accuracy: 0.8518 - val_loss: 1.2359 - val_accuracy: 0.6643\nEpoch 13/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.3771 - accuracy: 0.8664 - val_loss: 1.2986 - val_accuracy: 0.6655\nEpoch 14/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.3403 - accuracy: 0.8788 - val_loss: 1.3523 - val_accuracy: 0.6601\nEpoch 15/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.3023 - accuracy: 0.8937 - val_loss: 1.4420 - val_accuracy: 0.6636\nEpoch 16/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2727 - accuracy: 0.9042 - val_loss: 1.5756 - val_accuracy: 0.6633\nEpoch 17/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2488 - accuracy: 0.9102 - val_loss: 1.6647 - val_accuracy: 0.6542\nEpoch 18/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2274 - accuracy: 0.9197 - val_loss: 1.7585 - val_accuracy: 0.6649\nEpoch 19/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2063 - accuracy: 0.9265 - val_loss: 1.8737 - val_accuracy: 0.6562\nEpoch 20/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.1834 - accuracy: 0.9355 - val_loss: 1.9860 - val_accuracy: 0.6588\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"model2=Sequential()\nmodel2.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(Dropout(0.25)) # Drop 25% of the units from the layer.\nmodel2.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(Dropout(0.25))\nmodel2.add(Flatten())\nmodel2.add(Dense(256,activation='relu'))\nmodel2.add(Dense(10,activation='softmax'))\nmodel2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n# training_steps = train_generator.samples//train_generator.batch_size\n# validation_steps=test_generator.samples//test_generator.batch_size\n# model2.fit_generator(train_generator,epochs=100,steps_per_epoch=training_steps,validation_data=test_generator,validation_steps=validation_steps,callbacks=[board])\nhistory2=model2.fit(dataset['x_train'],dataset['y_train'],epochs=20,validation_data=(dataset['x_test'],dataset['y_test']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:29:58.115741Z","iopub.execute_input":"2025-02-26T20:29:58.116083Z","iopub.status.idle":"2025-02-26T20:32:04.755572Z","shell.execute_reply.started":"2025-02-26T20:29:58.116056Z","shell.execute_reply":"2025-02-26T20:32:04.754849Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"2025-02-26 20:30:00.571276: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_6/dropout_9/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n","output_type":"stream"},{"name":"stdout","text":"1563/1563 [==============================] - 9s 4ms/step - loss: 1.6039 - accuracy: 0.4139 - val_loss: 1.3765 - val_accuracy: 0.5180\nEpoch 2/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.3016 - accuracy: 0.5333 - val_loss: 1.1508 - val_accuracy: 0.5989\nEpoch 3/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.1598 - accuracy: 0.5903 - val_loss: 1.0574 - val_accuracy: 0.6274\nEpoch 4/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.0719 - accuracy: 0.6216 - val_loss: 0.9879 - val_accuracy: 0.6543\nEpoch 5/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.0032 - accuracy: 0.6480 - val_loss: 0.9901 - val_accuracy: 0.6570\nEpoch 6/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9419 - accuracy: 0.6687 - val_loss: 0.9262 - val_accuracy: 0.6822\nEpoch 7/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.8977 - accuracy: 0.6853 - val_loss: 0.9059 - val_accuracy: 0.6837\nEpoch 8/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.8594 - accuracy: 0.6987 - val_loss: 0.9032 - val_accuracy: 0.6924\nEpoch 9/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.8211 - accuracy: 0.7115 - val_loss: 0.9051 - val_accuracy: 0.6866\nEpoch 10/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.7934 - accuracy: 0.7209 - val_loss: 0.9216 - val_accuracy: 0.6834\nEpoch 11/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.7719 - accuracy: 0.7263 - val_loss: 0.8805 - val_accuracy: 0.7026\nEpoch 12/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.7401 - accuracy: 0.7374 - val_loss: 0.9114 - val_accuracy: 0.6879\nEpoch 13/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.7218 - accuracy: 0.7456 - val_loss: 0.8972 - val_accuracy: 0.6979\nEpoch 14/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.7022 - accuracy: 0.7523 - val_loss: 0.9051 - val_accuracy: 0.6970\nEpoch 15/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6830 - accuracy: 0.7592 - val_loss: 0.9040 - val_accuracy: 0.7031\nEpoch 16/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6729 - accuracy: 0.7626 - val_loss: 0.8811 - val_accuracy: 0.7034\nEpoch 17/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6585 - accuracy: 0.7673 - val_loss: 0.8959 - val_accuracy: 0.7033\nEpoch 18/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6426 - accuracy: 0.7720 - val_loss: 0.9054 - val_accuracy: 0.6995\nEpoch 19/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6269 - accuracy: 0.7789 - val_loss: 0.8885 - val_accuracy: 0.7106\nEpoch 20/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6182 - accuracy: 0.7828 - val_loss: 0.9142 - val_accuracy: 0.6991\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\ndef load_dataset(batch_size, num_classes, epochs):\n    # Load CIFAR-10 dataset\n    (x_train_full, y_train_full), (x_test, y_test) = cifar10.load_data()\n\n    # Normalize images (convert to float32 and scale to [0,1])\n    x_train_full = x_train_full.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\n\n\n    print('Shape of x_train is {}'.format(x_train_full.shape))\n    print('Shape of x_test is {}'.format(x_test.shape)) \n    print('Shape of y_train is {}'.format(y_train_full.shape))\n    print('Shape of y_test is {}'.format(y_test.shape))\n    \n    # Convert labels to one-hot encoding\n    y_train_full = to_categorical(y_train_full, num_classes)\n    y_test = to_categorical(y_test, num_classes)\n\n    print('Shape of y_train is {}'.format(y_train_full.shape))\n    print('Shape of y_test is {}'.format(y_test.shape))\n    \n    # Split training data into 40,000 for training and 10,000 for validation\n    x_train, x_val, y_train, y_val = train_test_split(\n        x_train_full, y_train_full, test_size=0.2, random_state=42\n    )\n\n    dataset = {\n        'batch_size': batch_size,\n        'num_classes': num_classes,\n        'epochs': epochs,\n        'x_train': x_train,\n        'y_train': y_train,\n        'x_val': x_val,\n        'y_val': y_val,\n        'x_test': x_test,  \n        'y_test': y_test\n    }\n\n    return dataset\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:40:10.450604Z","iopub.execute_input":"2025-02-26T20:40:10.451349Z","iopub.status.idle":"2025-02-26T20:40:10.458377Z","shell.execute_reply.started":"2025-02-26T20:40:10.451320Z","shell.execute_reply":"2025-02-26T20:40:10.457456Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"batch_size = 32                       # the number of training examples in one forward/backward pass\nnum_classes = 10                        # number of cifar-10 dataset classes\nepochs =20   \ndataset = load_dataset(batch_size, num_classes, epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:40:11.115033Z","iopub.execute_input":"2025-02-26T20:40:11.115395Z","iopub.status.idle":"2025-02-26T20:40:12.374861Z","shell.execute_reply.started":"2025-02-26T20:40:11.115367Z","shell.execute_reply":"2025-02-26T20:40:12.373960Z"}},"outputs":[{"name":"stdout","text":"Shape of x_train is (50000, 32, 32, 3)\nShape of x_test is (10000, 32, 32, 3)\nShape of y_train is (50000, 1)\nShape of y_test is (10000, 1)\nShape of y_train is (50000, 10)\nShape of y_test is (10000, 10)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}