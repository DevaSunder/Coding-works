{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30637,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"### UTILITIES\nfrom keras.utils import to_categorical\nimport pickle\nimport sys\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nfrom keras.datasets import cifar10\nfrom PIL import Image\nimport os\nimport numpy as np\n\n\n\"\"\"\ndef load_images_from_folder(folder_path, image_shape):\n    images = []\n    labels = []\n\n    for label, class_folder in enumerate(sorted(os.listdir(folder_path))):\n        class_path = os.path.join(folder_path, class_folder)\n        for filename in os.listdir(class_path):\n            img_path = os.path.join(class_path, filename)\n            img = load_image(img_path, image_shape)\n            images.append(img)\n            labels.append(label)\n\n    images = np.array(images)\n    labels = np.array(labels)\n    return images, labels\n\nimport numpy as np\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\ndef load_dataset(batch_size, num_classes, epochs):\n    # Load CIFAR-10 dataset\n    (x_train_full, y_train_full), (x_test, y_test) = cifar10.load_data()\n\n    # Normalize images (convert to float32 and scale to [0,1])\n    x_train_full = x_train_full.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\n\n    # Convert labels to one-hot encoding\n    y_train_full = to_categorical(y_train_full, num_classes)\n    y_test = to_categorical(y_test, num_classes)\n\n    # Split training data into 40,000 for training and 10,000 for validation\n    x_train, x_val, y_train, y_val = train_test_split(\n        x_train_full, y_train_full, test_size=0.2, random_state=42\n    )\n\n    dataset = {\n        'batch_size': batch_size,\n        'num_classes': num_classes,\n        'epochs': epochs,\n        'x_train': x_train,\n        'y_train': y_train,\n        'x_val': x_val,\n        'y_val': y_val,\n        'x_test': x_test,  \n        'y_test': y_test\n    }\n\n    return dataset\n\"\"\"\n\ndef save_network(network):\n    #object_file = open(network.name + '.obj', 'wb')\n    #pickle.dump(network, object_file)\n    #tf.keras.models.save_model(network, network.name)\n\n    model_path = network.name + '_model.h5'\n    tf.keras.models.save_model(network.model, model_path)\n\n    # Save the rest of the network information\n    network_info = {\n        'name': network.name,\n        'block_list': network.block_list,\n        'fitness': network.fitness\n    }\n    network_info_path = network.name + '_info.pkl'\n    with open(network_info_path, 'wb') as info_file:\n        pickle.dump(network_info, info_file)\n\n\ndef load_network(name):\n    model_path = name + '_model.h5'\n    loaded_model = tf.keras.models.load_model(model_path)\n\n    # Load the network information\n    info_path = name + '_info.pkl'\n    with open(info_path, 'rb') as info_file:\n        network_info = pickle.load(info_file)\n\n    # Create a new Network instance\n    loaded_network = Network(0)  # Update with appropriate 'it' value\n\n    # Set the attributes of the loaded network\n    loaded_network.name = network_info['name']\n    loaded_network.block_list = network_info['block_list']\n    loaded_network.fitness = network_info['fitness']\n    loaded_network.model = loaded_model\n\n    return loaded_network\n\n\n\ndef order_indexes(self):\n    i = 0\n    for block in self.block_list:\n        block.index = i\n        i += 1\n\n\ndef plot_training(history):                                           # plot diagnostic learning curves\n    plt.figure(figsize=[8, 6])  # accuracy curves\n    plt.plot(history.history['accuracy'], 'r', linewidth=3.0)\n    plt.plot(history.history['val_accuracy'], 'b', linewidth=3.0)  # <-- Change 'val_acc' to 'val_accuracy'\n    plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=18)\n    plt.xlabel('Epochs ', fontsize=16)\n    plt.ylabel('Accuracy', fontsize=16)\n    plt.title('Accuracy Curves', fontsize=16)\n\n    filename = sys.argv[0].split('/')[-1]\n    plt.savefig(filename + '_acc_plot.png')\n    plt.close()\n\n\n\ndef plot_statistics(stats):\n    plt.figure(figsize=[8, 6])\t\t\t\t\t\t\t\t\t\t\t# fitness curves\n    plt.plot([s[0] for s in stats], 'r', linewidth=3.0)\n    plt.plot([stats[0][0]] * len(stats), 'b', linewidth=3.0)\n    plt.legend(['BestFitness', 'InitialFitness'], fontsize=18)\n    plt.xlabel('Generations', fontsize=16)\n    plt.ylabel('FitnessValue', fontsize=16)\n    plt.title('Fitness Curve', fontsize=16)\n    filename = sys.argv[0].split('/')[-1]\n    plt.savefig(filename + '_fitness_plot.png')\n\n    plt.figure(figsize=[8, 6])\t\t\t\t\t\t\t\t\t\t\t# parameters curves\n    plt.plot([s[1] for s in stats], 'r', linewidth=3.0)\n    plt.plot([stats[0][1]] * len(stats), 'b', linewidth=3.0)\n    plt.legend(['BestParamsNum', 'InitialParamsNum'], fontsize=18)\n    plt.xlabel('Generations', fontsize=16)\n    plt.ylabel('ParamsNum', fontsize=16)\n    plt.title('Parameters Curve', fontsize=16)\n    filename = sys.argv[0].split('/')[-1]\n    plt.savefig(filename + '_params_plot.png')\n    plt.close()\n","metadata":{"execution":{"iopub.status.busy":"2025-02-27T13:38:43.537168Z","iopub.execute_input":"2025-02-27T13:38:43.537770Z","iopub.status.idle":"2025-02-27T13:38:43.553656Z","shell.execute_reply.started":"2025-02-27T13:38:43.537738Z","shell.execute_reply":"2025-02-27T13:38:43.552442Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"code","source":"#(Block number,position)\n# initial - 0, # middle -1, Final -2\n\n# INOUT\nimport os\ndef compute_parent(dataset):\n    if os.path.isfile('parent_0.h5'):\n        daddy = load_network('parent_0')\n        model = tf.keras.models.load_model('parent_0.h5')\n        print(\"Loading parent_0\")\n        print(\"SUMMARY OF\", daddy.name)\n        print(model.summary())\n        print(\"FITNESS:\", daddy.fitness)\n        return daddy\n\n    daddy = Network(0)\n\n\n    layerList1 = [\n        Convolutional(filters=64, stride_size=(1, 1),kernel_size=(3,3),input_shape=(32,32,3), padding='same'),\n        Convolutional(filters=64, stride_size=(1, 1),kernel_size=(3,3),input_shape=(32,32,3), padding='same')\n        \n    ]\n    layerList2 = [\n        Pooling(pool_size=(2, 2), stride_size=(2, 2), padding='same'),\n        Dropout(0.25)\n    ]\n    daddy.block_list.append(Block(0, 0, layerList1, layerList2))\n\n\n\n    \n    #MIDDLE BLOCK 1\n    layerList1 = [\n        Convolutional(filters=128, stride_size=(1, 1),kernel_size=(3,3),input_shape=(32,32,3), padding='same'),\n        Convolutional(filters=128, stride_size=(1, 1),kernel_size=(3,3),input_shape=(32,32,3), padding='same')\n        \n    ]\n    layerList2 = [\n        Pooling(pool_size=(2, 2), stride_size=(2, 2), padding='same'),\n        Dropout(0.25)\n    ]\n    daddy.block_list.append(Block(1, 1, layerList1, layerList2))\n\n\n\n\n    \n    #FULLY CONNECTED LAYER\n    layerList1 = [Flatten()]\n        \n    layerList2 = [FullyConnected(units=1024, num_classes=dataset['num_classes'])]\n    daddy.block_list.append(Block(2, 2, layerList1, layerList2))\n\n\n\n\n    \n    \"\"\"\n    #INI BLOCK\n    layerList1 = [\n        Convolutional(filters=32, stride_size=(1, 1),kernel_size=(4,4),input_shape=(32,32,3), padding='same'),\n        Pooling(pool_size=(2, 2), stride_size=(2, 2), padding='same')\n    ]\n    layerList2 = [\n        Dropout(0.25)\n    ]\n    daddy.block_list.append(Block(0, 0, layerList1, layerList2))\n    \n    #MIDDLE BLOCK 1\n    layerList1 = [\n        Convolutional(filters=32, filter_size=(3, 3), stride_size=(1, 1), padding='same',\n                      input_shape=dataset['x_train'].shape[1:]),\n        Pooling(pool_size=(2, 2), stride_size=(2, 2), padding='same')\n    ]\n    layerList2 = [\n        Dropout(0.25)\n    ]\n    daddy.block_list.append(Block(1, 1, layerList1, layerList2))\n\n    #FULLY CONNECTED LAYER\n    layerList1 = [\n        FullyConnected(units=128, num_classes=dataset['num_classes'])\n    ]\n    layerList2 = []\n    daddy.block_list.append(Block(2, 5, layerList1, layerList2))\n    \"\"\"\n    \n\n    model = daddy.build_model()\n    print(\"Type of model_final:\", type(model))\n    daddy.train_and_evaluate(model, dataset)\n    return daddy","metadata":{"execution":{"iopub.status.busy":"2025-02-27T13:38:45.221747Z","iopub.execute_input":"2025-02-27T13:38:45.222507Z","iopub.status.idle":"2025-02-27T13:38:45.232416Z","shell.execute_reply.started":"2025-02-27T13:38:45.222478Z","shell.execute_reply":"2025-02-27T13:38:45.231495Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# NETWORK\nimport tensorflow as tf\nimport os\nimport pickle\nfrom keras.callbacks import Callback\nfrom keras.models import Sequential\nfrom random import randint, choice\nfrom copy import deepcopy\n\n\nclass Network:\n    __slots__ = ('name', 'block_list', 'fitness', 'model')\n\n    def __init__(self, it):\n        self.name = 'parent_' + str(it) if it == 0 else 'net_' + str(it)\n        self.block_list = []\n        self.fitness = None\n        self.model = None\n\n    \"\"\"def build_model(self):\n        model = Sequential()                                # create model\n        for block in self.block_list:\n            for layer in block.get_layers():                # build model\n                try:\n                    layer.build_layer(model)\n                except:\n                    print(\"\\nINDIVIDUAL ABORTED, CREATING A NEW ONE\\n\")\n                    return -1\n        return model\"\"\"\n    def build_model(self):\n        model = Sequential()              \n        print(\"The block is:\")\n        print(self.block_list)                 # create model\n        for block in self.block_list:\n            #print(\"Building block type:\", block.type)\n            #print(\"TOTAL :::\")\n            #print(block.get_layer_name())\n            for layer in block.get_layers():                # build model\n                #print(\"Adding layer:\", layer.name)\n                try:\n                    layer.build_layer(model)\n                    print(\"Layer added successfully.\")\n                except Exception as e:\n                    print(\"Error occurred while adding layer:\", e)\n                    print(\"Returning None.\")\n                    return -1\n        print(\"Model successfully built.\")\n        return model\n\n    def train_and_evaluate(self, model, dataset):\n        print(\"Training\", self.name)\n        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n        try:\n            history = model.fit(dataset['x_train'],\n                                dataset['y_train'],\n                                epochs=dataset['epochs'],\n                                validation_data=(dataset['x_test'], dataset['y_test']),\n                                shuffle=True)\n        except Exception as e:\n            print(\"An error occurred during model training:\", e)\n            return -1\n            # You can choose to handle the error in a specific way here, like logging it or taking corrective actions.\n\n\n        # Extract metrics from the training history\n        training_loss = history.history['loss'][-1]\n        training_accuracy = history.history['accuracy'][-1]\n        validation_loss = history.history['val_loss'][-1]\n        validation_accuracy = history.history['val_accuracy'][-1]\n\n        print(model.summary())\n        x= input(\"Do you want to continue ?\")\n        # Additional metrics (you can customize this based on your needs)\n        classification_error_rate = 1.0 - validation_accuracy\n\n        self.model = model  # Save the model\n        self.fitness = validation_loss  # Use validation loss as fitness\n\n        # Print metrics\n        print(\"SUMMARY OF\", self.name)\n        print(\"Training Loss:\", training_loss)\n        print(\"Training Accuracy:\", training_accuracy)\n        print(\"Validation Loss:\", validation_loss)\n        print(\"Validation Accuracy:\", validation_accuracy)\n        print(\"Classification Error Rate:\", classification_error_rate)\n\n        tf.keras.models.save_model(model, self.name + '.h5')         # save model\n        #model.save(self.name + '.h5')                       # save model\n        save_network(self)                                  # save topology, model and fitness\n\n    def asexual_reproduction(self, it, dataset):\n\n        # if the individual already exists, just load it\n        if os.path.isfile('net_' + str(it) + '.h5'):\n            print(\"\\n-------------------------------------\")\n            print(\"Loading individual net_\" + str(it))\n            print(\"--------------------------------------\\n\")\n            individual = load_network('net_' + str(it))\n            model = tf.keras.models.load_model(individual.name + '.h5')\n            print(\"SUMMARY OF\", individual.name)\n            print(model.summary())\n            print(\"FITNESS: \", individual.fitness)\n            return individual\n\n        # otherwise, create the individual by mutating the parent\n        individual = Network(it)\n\n        print(\"\\n-------------------------------------\")\n        print(\"\\nCreating individual\", individual.name)\n        print(\"--------------------------------------\\n\")\n\n        individual.block_list = deepcopy(self.block_list)           # copy the layer list from parent\n\n        print(\"----->Strong Mutation\")\n        individual.block_mutation(dataset)                          # mutate a block\n        individual.layer_mutation(dataset)                          # mutate a layer\n        individual.parameters_mutation()                            # mutate some parameters\n\n        model = individual.build_model()\n        \n        if model == -1:\n            return self.asexual_reproduction(it, dataset)\n        \n        if(individual.train_and_evaluate(model, dataset)==-1):\n            return self.asexual_reproduction(it, dataset)\n        else:\n            return individual\n            \n\n    def block_mutation(self, dataset):\n        try:\n            print(\"Block Mutation\")\n\n            print([(block.index, block.type) for block in self.block_list])\n\n            # block list containing all the blocks with type = 1\n            bl = [block.index for block in self.block_list if block.type == 1]\n\n            if len(bl) == 0:\n                print(\"Creating a new block with two Convolutional layers and a Pooling layer\")\n                self.block_list[1].index = 2\n                layerList1 = [\n                    Convolutional(filters=pow(2, randint(5, 8)),\n                                  kernel_size=(3, 3),\n                                  stride_size=(1, 1),\n                                  padding='same',\n                                  input_shape=dataset['x_train'].shape[1:]),\n                    Convolutional(filters=pow(2, randint(5, 8)),\n                                  kernel_size=(3, 3),\n                                  stride_size=(1, 1),\n                                  padding='same',\n                                  input_shape=dataset['x_train'].shape[1:])\n                ]\n                layerList2 = [\n                    Pooling(pool_size=(2, 2),\n                            stride_size=(2, 2),\n                            padding='same')\n                ]\n                b = Block(1, 1, layerList1, layerList2)\n                self.block_list.insert(1, b)\n                return\n\n            block_idx = randint(1, max(bl))         # pick a random block among all the blocks with type = 1\n            block_type_idx = randint(0, 1)          # 1 -> Conv2D; 0 -> Pooling or Dropout\n            mutation_type = randint(0, 1)           # 1 -> remove; 0 -> add\n\n            # list of layers of the selected block\n            layerList = self.block_list[block_idx].layerList1 if block_type_idx else self.block_list[block_idx].layerList2\n            length = len(layerList)\n\n            if mutation_type:                                       # remove\n                if length == 1:\n                    del self.block_list[block_idx]\n                elif block_type_idx:\n                    pos = randint(0, length - 1)\n                    print(\"Removing a Conv2D layer at\", pos)\n                    del layerList[pos]\n                else:\n                    pos = randint(0, length - 1)\n                    print(\"Removing a Pooling/Dropout layer at\", pos)\n                    del layerList[pos]\n            else:                                                   # add\n                if block_type_idx:\n                    print(\"Inserting a Convolutional layer\")\n                    layer = Convolutional(filters=pow(2, randint(5, 8)),\n                                          kernel_size=(3, 3),\n                                          stride_size=(1, 1),\n                                          padding='same',\n                                          input_shape=dataset['x_train'].shape[1:])\n                    layerList.insert(randint(0, length - 1), layer)\n                else:\n                    if randint(0, 1):                               # 1 -> Pooling; 0 -> Dropout\n                        print(\"Inserting a Pooling layer\")\n                        layer = Pooling(pool_size=(2, 2),\n                                        stride_size=(2, 2),\n                                        padding='same')\n                        layerList.insert(randint(0, length - 1), layer)\n                    else:\n                        print(\"Inserting a Dropout layer\")\n                        rate = choice([0.15, 0.25, 0.35, 0.50])\n                        layer = Dropout(rate=rate)\n                        layerList.insert(randint(0, length - 1), layer)\n        except Exception as e:\n            print(f\"An error occurred during block mutation: {e}\")\n            return None\n\n                    \n                    \n                    \n                    \n                    \n\n    \"\"\"def layer_mutation(self, dataset):\n        print(\"Layer Mutation\")\n\n        # pick a random block among all the blocks with type = 1\n        bl = [block.index for block in self.block_list if block.type == 1]\n\n        if len(bl) == 0:\n            return\n\n        block_idx = randint(1, max(bl))\n        block_type_idx = randint(0, 1)      # 1 -> Conv2D; 0 -> Pooling or Dropout\n\n        # list of layers of the selected block\n        layerList = self.block_list[block_idx].layerList1 if block_type_idx else self.block_list[block_idx].layerList2\n\n        if len(layerList) == 0:\n            if block_type_idx:\n                layer = Convolutional(filters=pow(2, randint(5, 8)),\n                                      filter_size=(3, 3),\n                                      stride_size=(1, 1),\n                                      padding='same',\n                                      input_shape=dataset['x_train'].shape[1:])\n                self.block_list[block_idx].layerList1.append(layer)\n                return\n            else:\n                layer = Pooling(pool_size=(2, 2),\n                                stride_size=(2, 2),\n                                padding='same')\n                self.block_list[block_idx].layerList2.append(layer)\n\n        idx = randint(0, len(layerList) - 1)\n        layer = layerList[idx]\n\n        if layer.name == 'Conv2D':\n            print(\"Splitting Conv2D layer at index\", idx)\n            layer.filters = int(layer.filters * 0.5)\n            layerList.insert(idx, deepcopy(layer))\n        elif layer.name == 'MaxPooling2D' or layer.name == 'AveragePooling2D':\n            print(\"Changing Pooling layer at index\", idx, \"with Conv2D layer\")\n            del layerList[idx]\n            conv_layer = Convolutional(filters=pow(2, randint(5, 8)),\n                                       filter_size=(3, 3),\n                                       stride_size=(2, 2),\n                                       padding=layer.padding,\n                                       input_shape=dataset['x_train'].shape[1:])\n            layerList.insert(idx, conv_layer)\"\"\"\n    \n    def layer_mutation(self, dataset):\n        print(\"Layer Mutation\")\n\n        # Determine the maximum number of layers that can be added or removed\n        max_layers_to_add = 16 - sum(len(block.layerList1) + len(block.layerList2) for block in self.block_list)\n        max_layers_to_remove = sum(len(block.layerList1) + len(block.layerList2) - 1 for block in self.block_list)\n\n        if max_layers_to_add == 0 and max_layers_to_remove == 0:\n            return\n\n        # Pick a random block among all the blocks with type = 1\n        bl = [block.index for block in self.block_list if block.type == 1]\n\n        if len(bl) == 0:\n            return\n\n        block_idx = randint(1, max(bl))\n        block_type_idx = randint(0, 1)      # 1 -> Conv2D; 0 -> Pooling or Dropout\n\n        # List of layers of the selected block\n        layerList = self.block_list[block_idx].layerList1 if block_type_idx else self.block_list[block_idx].layerList2\n\n        if len(layerList) == 0:\n            if block_type_idx:\n                layer = Convolutional(filters=pow(2, randint(5, 8)),\n                                      kernel_size=(3, 3),\n                                      stride_size=(1, 1),\n                                      padding='same',\n                                      input_shape=dataset['x_train'].shape[1:])\n                self.block_list[block_idx].layerList1.append(layer)\n            else:\n                layer = Pooling(pool_size=(2, 2),\n                                stride_size=(2, 2),\n                                padding='same')\n                self.block_list[block_idx].layerList2.append(layer)\n        else:\n            # Randomly choose whether to add or remove a layer\n            add_layer = bool(randint(0, 1))\n\n            if add_layer and max_layers_to_add > 0:\n                # Add a layer\n                layer = self.create_random_layer(dataset)\n                layerList.insert(randint(0, len(layerList)), layer)\n            elif not add_layer and max_layers_to_remove > 0:\n                # Remove a layer\n                idx = randint(0, len(layerList) - 1)\n                del layerList[idx]\n\n        # Ensure the total number of layers in the block doesn't exceed 16\n        if len(self.block_list[block_idx].layerList1) + len(self.block_list[block_idx].layerList2) > 16:\n            # Remove a random layer to maintain the total count of 16 layers\n            block_layerList = self.block_list[block_idx].layerList1 if block_type_idx else self.block_list[block_idx].layerList2\n            del block_layerList[randint(0, len(block_layerList) - 1)]\n\n    def create_random_layer(self, dataset):\n        # Create a random layer (Conv2D or Pooling)\n        if randint(0, 1):\n            # Conv2D layer\n            return Convolutional(filters=pow(2, randint(5, 8)),\n                                 kernel_size=(3, 3),\n                                 stride_size=(1, 1),\n                                 padding='same',\n                                 input_shape=dataset['x_train'].shape[1:])\n        else:\n            # Pooling layer\n            return Pooling(pool_size=(2, 2),\n                           stride_size=(2, 2),\n                           padding='same')\n\n            \n            \n            \n            \n            \n            \n            \n            \n\n    def parameters_mutation(self):\n        print(\"Parameters Mutation\")\n        for block in self.block_list:\n            for layer in block.get_layers():\n                if randint(0, 1):\n                    layer.mutate_parameters()\n\n    def save_network_info(self, info_filename):\n        network_info = {\n            'name': self.name,\n            'block_list': self.block_list,\n            'fitness': self.fitness\n        }\n\n        with open(info_filename, 'wb') as info_file:\n            pickle.dump(network_info, info_file)\n\n    def load_network_info(self, info_filename):\n        with open(info_filename, 'rb') as info_file:\n            network_info = pickle.load(info_file)\n\n        self.name = network_info['name']\n        self.block_list = network_info['block_list']\n        self.fitness = network_info['fitness']\n\n    def save_model(self, model_filename):\n        self.model.save(model_filename)\n\n    def load_model(self, model_filename):\n        self.model = tf.keras.models.load_model(model_filename)\n\n    def save_network(self, network_info_filename, model_filename):\n        # Save non-model attributes\n        self.save_network_info(network_info_filename)\n\n        # Save the model separately\n        self.save_model(model_filename)\n\n    def load_network(self, network_info_filename, model_filename):\n        # Load non-model attributes\n        self.load_network_info(network_info_filename)\n\n        # Load the model separately\n        self.load_model(model_filename)\n","metadata":{"execution":{"iopub.status.busy":"2025-02-27T13:38:46.991890Z","iopub.execute_input":"2025-02-27T13:38:46.992202Z","iopub.status.idle":"2025-02-27T13:38:47.025950Z","shell.execute_reply.started":"2025-02-27T13:38:46.992178Z","shell.execute_reply":"2025-02-27T13:38:47.025257Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# TOPOLOGY\n\nimport keras.layers\nfrom random import randint\n\n\nclass Block:\n\t__slots__ = ('type', 'index', 'layerList1', 'layerList2')\n\n\tdef __init__(self, type, index, layerList1, layerList2):\n\t\tself.type = type\t\t\t\t\t\t\t\t\t\t# 0 -> initial layer; 1 -> mid layers; 2 -> final layer\n\t\tself.index = index\t\t\t\t\t\t\t\t\t\t# block index among all the blocks\n\t\tself.layerList1 = layerList1\t\t\t\t\t\t\t# Convolutional layers\n\t\tself.layerList2 = layerList2\t\t\t\t\t\t\t# Pooling and Dropout layers\n\n\tdef get_layers(self):\n\t\treturn self.layerList1 + self.layerList2\n\n\tdef get_size(self):\n\t\treturn len(self.get_layers())\n\n\nclass Convolutional:\n\t# __slots__ = ('name', 'filters', 'padding', 'filter_size', 'stride_size', 'input_shape')\n\n\tdef __init__(self, filters, padding,stride_size, input_shape, kernel_size):\n\t\tself.name = 'Conv2D'\n\t\tself.filters = filters\n\t\tself.padding = padding\n\t\tself.kernel_size = kernel_size\n\t\tself.stride_size = stride_size\n\t\tself.input_shape = input_shape\n        \n\n\tdef build_layer(self, model):\n\t\ttry:\n\t\t\tmodel.add(keras.layers.Conv2D(filters=self.filters,\n\t\t\t\t\t\t\t\t\t\t\tkernel_size=self.kernel_size,\n\t\t\t\t\t\t\t\t\t\t\tstrides=self.stride_size,\n\t\t\t\t\t\t\t\t\t\t\tpadding=self.padding,\n\t\t\t\t\t\t\t\t\t\t\tactivation='relu',\n\t\t\t\t\t\t\t\t\t\t\tkernel_initializer='he_uniform',\n\t\t\t\t\t\t\t\t\t\t\tinput_shape=self.input_shape))\n\t\texcept ValueError as e:\n\t\t\tprint(\"Error occurred while adding layer:\", e)\n\t\t\tprint(\"Skipping current architecture.\")\n\t\t\treturn  # Skip adding this layer\n\tdef mutate_parameters(self):\n\t\tmutation = randint(0, 2)  # Adjusted the number of mutations\n\t\tprint(\"Mutating\", self.name, \"layer:\")\n\t\tif mutation == 0 and self.filters >= 64:  # Adjusted the filter reduction threshold\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters = int(self.filters / 2)\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 1 and self.filters <= 256:  # Adjusted the filter increase threshold\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters *= 2\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 2:\n\t\t\tif self.padding == 'valid':\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'same'\n\t\t\t\tprint(\"to \", self.padding)\n\t\t\telse:\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'valid'\n\t\t\t\tprint(\"to \", self.padding)\n\n        \n\n        \n\n\n\t\"\"\"def mutate_parameters(self):\n\t\tmutation = randint(0, 4)\n\t\tprint(\"Mutating\", self.name, \"layer:\")\n\t\tif mutation == 0 and self.filters >= 32:\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters = int(self.filters / 2)\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 1 and self.filters >= 32:\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters = int(self.filters / 2)\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 2 and self.filters <= 512:\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters *= 2\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 3 and self.filters <= 512:\n\t\t\tprint(\"-->changed self.filters from \", self.filters, \" \", end=\"\")\n\t\t\tself.filters *= 2\n\t\t\tprint(\"to \", self.filters)\n\t\telif mutation == 4:\n\t\t\tif self.padding == 'valid':\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'same'\n\t\t\t\tprint(\"to \", self.padding)\n\t\t\telse:\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'valid'\n\t\t\t\tprint(\"to \", self.padding)\n\"\"\"\n    \n\n'''\nelif mutation is 4:\n\tprint(\"changed self.stride_size from \", self.stride_size, \" \", end=\"\")\n\tself.stride_size = (self.stride_size[0] + 1, self.stride_size[1] + 1)\n\tprint(\"to \", self.stride_size, \" and \", end=\"\")\n\tprint(\"changed self.stride_size from \", self.stride_size, \" \", end=\"\")\n\tself.stride_size = (self.stride_size[0] + 1, self.stride_size[1] + 1)\n\tprint(\"to \", self.stride_size)\n'''\n\n\nclass Pooling:\n\t__slots__ = ('name', 'pool_size', 'stride_size', 'padding')\n\n\tdef __init__(self, pool_size, stride_size, padding):\n\t\tself.name = 'MaxPooling2D'\n\t\tself.pool_size = pool_size\n\t\tself.stride_size = stride_size\n\t\tself.padding = padding\n\n\tdef build_layer(self, model):\n\t\tif self.name == 'MaxPooling2D':\n\t\t\tmodel.add(keras.layers.MaxPooling2D(self.pool_size, self.stride_size, self.padding))\n\t\telif self.name == 'AveragePooling2D':\n\t\t\tmodel.add(keras.layers.AveragePooling2D(self.pool_size, self.stride_size, self.padding))\n\n\tdef mutate_parameters(self):\n\t\tprint(\"Mutating\", self.name, \"layer:\")\n\t\tmutation = randint(0, 1)\n\t\tif mutation == 0:\n\t\t\tif self.padding == 'valid':\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'same'\n\t\t\t\tprint(\"to \", self.padding)\n\t\t\telse:\n\t\t\t\tprint(\"-->changed self.padding from \", self.padding, \" \", end=\"\")\n\t\t\t\tself.padding = 'valid'\n\t\t\t\tprint(\"to \", self.padding)\n\t\telif mutation == 1:\n\t\t\tif self.name == 'MaxPooling2D':\n\t\t\t\tprint(\"-->changed self.name from \", self.name, \" \", end=\"\")\n\t\t\t\tself.name = 'AveragePooling2D'\n\t\t\t\tprint(\"to \", self.name)\n\t\t\telse:\n\t\t\t\tprint(\"-->changed self.name from \", self.name, \" \", end=\"\")\n\t\t\t\tself.name = 'MaxPooling2D'\n\t\t\t\tprint(\"to \", self.name)\n\n\n'''\nif mutation is 0:\n\tprint(\"changed self.stride_size from \", self.stride_size, \" \", end=\"\")\n\tself.stride_size = (self.stride_size[0] + 1, self.stride_size[1] + 1)\n\tprint(\"to \", self.stride_size)\n'''\n\n\nclass FullyConnected:\n\t__slots__ = ('name', 'units', 'num_classes')\n\n\tdef __init__(self, units, num_classes):\n\t\tself.name = \"FullyConnected\"\n\t\tself.units = units\n\t\tself.num_classes = num_classes\n\n\tdef build_layer(self, model):\n\t\tmodel.add(keras.layers.Flatten())\n\t\tmodel.add(keras.layers.Dense(self.num_classes, activation='softmax'))\n        model.add(keras.layers.Dense(self.units, activation='relu', kernel_initializer='he_uniform'))\n\n\n    \n\tdef mutate_parameters(self):\n\t\tprint(\"Mutating\", self.name, \"layer:\")\n\t\tmutation = randint(0, 2)\n\t\tif mutation == 0:\n\t\t\tprint(\"-->changed self.units from \", self.units, \" \", end=\"\")\n\t\t\tself.units *= 2\n\t\t\tprint(\"to \", self.units)\n\t\telif mutation == 1:\n\t\t\tprint(\"-->changed self.units from \", self.units, \" \", end=\"\")\n\t\t\tself.units *= 2\n\t\t\tprint(\"to \", self.units)\n\t\telif mutation == 2:\n\t\t\tprint(\"-->changed self.units from \", self.units, \" \", end=\"\")\n\t\t\tself.units /= 2\n\t\t\tprint(\"to \", self.units)\n\n\n'''\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(self.num_classes, activation='softmax'))\n'''\n\n\nclass Dropout:\n\t__slots__ = ('name', 'rate')\n\n\tdef __init__(self, rate):\n\t\tself.name = \"Dropout\"\n\t\tself.rate = rate\n\n\tdef build_layer(self, model):\n\t\tmodel.add(keras.layers.Dropout(self.rate))\n\n\tdef mutate_parameters(self):\n\t\tprint(\"Mutating\", self.name, \"layer:\")\n\t\tmutation = randint(0, 3)\n\t\tif mutation == 0 and self.rate <= 0.85:\n\t\t\tprint(\"-->changed self.rate from \", self.rate, \" \", end=\"\")\n\t\t\tself.rate = self.rate + 0.10\n\t\t\tprint(\"to \", self.rate)\n\t\telif mutation == 1 and self.rate <= 0.90:\n\t\t\tprint(\"-->changed self.rate from \", self.rate, \" \", end=\"\")\n\t\t\tself.rate = self.rate + 0.05\n\t\t\tprint(\"to \", self.rate)\n\t\telif mutation == 2 and self.rate >= 0.15:\n\t\t\tprint(\"-->changed self.rate from \", self.rate, \" \", end=\"\")\n\t\t\tself.rate = self.rate - 0.10\n\t\t\tprint(\"to \", self.rate)\n\t\telif mutation == 3 and self.rate >= 0.10:\n\t\t\tprint(\"-->changed self.rate from \", self.rate, \" \", end=\"\")\n\t\t\tself.rate = self.rate - 0.05\n\t\t\tprint(\"to \", self.rate)\n\nclass FlattenLayer:\n    def __init__(self):\n        self.name = 'Flatten'\n\n    def build_layer(self, model):\n        model.add(keras.layers.Flatten())\n\n    def mutate_parameters(self):\n        # The Flatten layer does not have any parameters to mutate\n        pass\n","metadata":{"execution":{"iopub.status.busy":"2025-02-27T13:47:17.126045Z","iopub.execute_input":"2025-02-27T13:47:17.126403Z","iopub.status.idle":"2025-02-27T13:47:17.145099Z","shell.execute_reply.started":"2025-02-27T13:47:17.126376Z","shell.execute_reply":"2025-02-27T13:47:17.144030Z"},"trusted":true},"outputs":[{"traceback":["\u001b[0;36m  Cell \u001b[0;32mIn[18], line 173\u001b[0;36m\u001b[0m\n\u001b[0;31m    model.add(keras.layers.Dense(self.units, activation='relu', kernel_initializer='he_uniform'))\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"],"ename":"TabError","evalue":"inconsistent use of tabs and spaces in indentation (966787715.py, line 173)","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"import tensorflow as tf\ntf.compat.v1.enable_eager_execution()\nimport os\nfrom copy import deepcopy\nfrom random import sample\ntf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)      # suppress messages from Tensorflow\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n\n\ndef initialize_population(population_size, dataset):\n    print(\"----->Initializing Population\")\n    daddy = compute_parent(dataset)                                 # load parent from input\n    population = [daddy]\n    for it in range(1, population_size):\n        population.append(daddy.asexual_reproduction(it, dataset))\n\n    # sort population on ascending order based on fitness\n    return sorted(population, key=lambda cnn: cnn.fitness)\n\n\ndef selection(k, population, num_population):\n    if k == 0:                                              # elitism selection\n        print(\"----->Elitism selection\")\n        return population[0], population[1]\n    elif k == 1:                                            # tournament selection\n        print(\"----->Tournament selection\")\n        i = randint(0, num_population - 1)\n        j = i\n        while j < num_population - 1:\n            j += 1\n            if randint(1, 100) <= 50:\n                return population[i], population[j]\n        return population[i], population[0]\n    else:                                                   # proportionate selection\n        print(\"----->Proportionate selection\")\n        cum_sum = 0\n        for i in range(num_population):\n            cum_sum += population[i].fitness\n        perc_range = []\n        for i in range(num_population):\n            count = 100 - int(100 * population[i].fitness / cum_sum)\n            for j in range(count):\n                perc_range.append(i)\n        i, j = sample(range(1, len(perc_range)), 2)\n        while i == j:\n            i, j = sample(range(1, len(perc_range)), 2)\n        return population[perc_range[i]], population[perc_range[j]]\n\n\ndef crossover(parent1, parent2, it):\n    print(\"----->Crossover\")\n    child = Network(it)\n\n    first, second = None, None\n    if randint(0, 1):\n        first = parent1\n        second = parent2\n    else:\n        first = parent2\n        second = parent1\n\n    child.block_list = deepcopy(first.block_list[:randint(1, len(first.block_list) - 1)]) \\\n                       + deepcopy(second.block_list[randint(1, len(second.block_list) - 1):])\n\n    order_indexes(child)                            # order the indexes of the blocks\n\n    return child\n\n\ndef genetic_algorithm(num_population, num_generation, num_offspring, dataset, early_stopping_generations=3):\n    print(\"Genetic Algorithm\")\n\n    population = initialize_population(num_population, dataset)\n\n    print(\"\\n-------------------------------------\")\n    print(\"Initial Population:\")\n    for cnn in population:\n        print(cnn.name, ': ', cnn.fitness)\n    print(\"--------------------------------------\\n\")\n\n    # for printing statistics about fitness and the number of parameters of the best individual\n    stats = [(population[0].fitness, population[0].model.count_params())]\n\n    # Initialize a variable to keep track of consecutive generations with the same best fitness\n    consecutive_same_fitness = 0\n\n    for gen in range(1, num_generation + 1):\n        '''\n            k is the selection parameter:\n                k = 0 -> elitism selection\n                k = 1 -> tournament selection\n                k = 2 -> proportionate selection\n        '''\n        k = randint(0, 2)\n\n        print(\"\\n------------------------------------\")\n        print(\"Generation -----------------------------------------------------------------------------------\", gen)\n        print(\"-------------------------------------\")\n\n        for c in range(num_offspring):\n\n            print(\"\\nCreating Child\", c)\n\n            parent1, parent2 = selection(k, population, num_population)                 # selection\n            print(\"Selected\", parent1.name, \"and\", parent2.name, \"for reproduction\")\n\n            child = crossover(parent1, parent2, c + num_population)                     # crossover\n            print(\"Child has been created\")\n\n            print(\"----->Soft Mutation\")\n            child.layer_mutation(dataset)                                               # mutation\n            child.parameters_mutation()\n            print(\"Child has been mutated\")\n\n            model = child.build_model()                                                 # evaluation\n\n            while model == -1:\n                child = crossover(parent1, parent2, c + num_population)\n                child.block_mutation(dataset)\n                child.layer_mutation(dataset)\n                child.parameters_mutation()\n                model = child.build_model()\n\n            child.train_and_evaluate(model, dataset)\n\n            if child.fitness < population[-1].fitness:                                  # evolve population\n                print(\"----->Evolution: Child\", child.name, \"with fitness\", child.fitness, \"replaces parent \", end=\"\")\n                print(population[-1].name, \"with fitness\", population[-1].fitness)\n                name = population[-1].name\n\n                child.save_network(\"child_model_info.pkl\", \"child_model.h5\")\n                population[-1].load_network(\"child_model_info.pkl\", \"child_model.h5\")\n\n                population[-1].name = name\n                population = sorted(population, key=lambda net: net.fitness)\n            else:\n                print(\"----->Evolution: Child\", child.name, \"with fitness\", child.fitness, \"is discarded\")\n        \n        if gen >= 3 and all(population[i].fitness == population[i + 1].fitness for i in range(-3, -1)):\n            consecutive_same_fitness += 1\n            print(f\"Consecutive generations with the same best fitness: {consecutive_same_fitness}\")\n        if consecutive_same_fitness >= 3:\n            print(\"Stopping the algorithm as the best fitness has remained the same for the last 3 generations.\")\n            break\n    else:\n        consecutive_same_fitness = 0\n        \n       #Check if the best fitness has remained the same for the last early_stopping_generations generations\n        if all(population[i].fitness == population[i + 1].fitness for i in range(-early_stopping_generations, -1)):\n            consecutive_same_fitness += 1\n            print(f\"Consecutive generations with the same best fitness: {consecutive_same_fitness}\")\n            if consecutive_same_fitness == early_stopping_generations:\n                print(f\"Stopping the algorithm as the best fitness has remained the same for {early_stopping_generations} generations.\")\n        else:\n            consecutive_same_fitness = 0\n        stats.append((population[0].fitness, population[0].model.count_params()))\n\n    print(\"\\n\\n-------------------------------------\")\n    print(\"Final Population\")\n    print(\"-------------------------------------\\n\")\n    for cnn in population:\n        print(cnn.name, ': ', cnn.fitness)\n\n    print(\"\\n-------------------------------------\")\n    print(\"Stats\")\n    for i in range(len(stats)):\n        print(\"Best individual at generation\", i + 1, \"has fitness\", stats[i][0], \"and parameters\", stats[i][1])\n    print(\"-------------------------------------\\n\")\n\n    # plot the fitness and the number of parameters of the best individual at each iteration\n    plot_statistics(stats)\n\n    return population[0]\n\n\n\ndef main(): \n        \n        # Importing the necessary libraries, which we may or may not use. Its always good idea to import them befor (if you remember) else we can do it at any point of time no problem.\n        from tensorflow.keras.models import Sequential\n        from tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten,Dropout,Input, AveragePooling2D, Activation,Conv2D, MaxPooling2D, BatchNormalization,Concatenate\n        from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n        from tensorflow.keras import regularizers, optimizers\n        from tensorflow.keras.preprocessing.image import ImageDataGenerator\n        from tensorflow.keras.datasets import cifar10\n        \n        (x_train, y_train), (x_test, y_test)=cifar10.load_data()\n        \n        \n        from tensorflow.keras.utils import to_categorical\n        \n        # Normalizing\n        x_train=x_train/255\n        x_test=x_test/255\n        \n        #One hot encoding\n        y_train_cat=to_categorical(y_train,10)\n        y_test_cat=to_categorical(y_test,10)\n\n        dataset = {\n        'num_classes': 10,\n        'epochs': 20,\n        'x_train': x_train,\n        'y_train': y_train_cat,\n        'x_test': x_test,\n        'y_test': y_test_cat\n    }\n\n\n        num_population = 10\n        num_generation = 10\n        num_offspring = 4\n\n        # plot the best model obtained\n        optCNN = genetic_algorithm(num_population, num_generation, num_offspring, dataset)\n\n        # plot the training and validation loss and accuracy\n        num_epoch = 20\n        model = optCNN.build_model()\n        model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n        history = model.fit(dataset['x_train'],\n                            dataset['y_train'],\n                            batch_size=dataset['batch_size'],\n                            epochs=num_epoch,\n                            validation_data=(dataset['x_test'], dataset['y_test']),\n                            shuffle=True)\n        optCNN.model = model                                        # model\n        optCNN.fitness = history.history['val_loss'][-1]            # fitness\n\n        print(\"\\n\\n-------------------------------------\")\n        print(\"The Final CNN has been evolved successfully in the individual\", optCNN.name)\n        print(\"-------------------------------------\\n\")\n        daddy = load_network('parent_0')\n        model = tf.keras.models.load_model('parent_0.h5')\n        print(\"\\n\\n-------------------------------------\")\n        print(\"Summary of initial CNN\")\n        print(model.summary())\n        print(\"Fitness of initial CNN:\", daddy.fitness)\n\n        print(\"\\n\\n-------------------------------------\")\n        print(\"Summary of evolved individual\")\n        print(optCNN.model.summary())\n        print(\"Fitness of the evolved individual:\", optCNN.fitness)\n        print(\"-------------------------------------\\n\")\n\n        plot_training(history)\n\n\nif __name__ == '__main__':\n    main()","metadata":{"execution":{"iopub.status.busy":"2025-02-27T13:38:49.777032Z","iopub.execute_input":"2025-02-27T13:38:49.777380Z","iopub.status.idle":"2025-02-27T13:38:57.211112Z","shell.execute_reply.started":"2025-02-27T13:38:49.777345Z","shell.execute_reply":"2025-02-27T13:38:57.209966Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n170498071/170498071 [==============================] - 3s 0us/step\nGenetic Algorithm\n----->Initializing Population\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 250\u001b[0m\n\u001b[1;32m    246\u001b[0m         plot_training(history)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[6], line 215\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m num_offspring \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# plot the best model obtained\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m optCNN \u001b[38;5;241m=\u001b[39m \u001b[43mgenetic_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_population\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_offspring\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# plot the training and validation loss and accuracy\u001b[39;00m\n\u001b[1;32m    218\u001b[0m num_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n","Cell \u001b[0;32mIn[6], line 73\u001b[0m, in \u001b[0;36mgenetic_algorithm\u001b[0;34m(num_population, num_generation, num_offspring, dataset, early_stopping_generations)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenetic_algorithm\u001b[39m(num_population, num_generation, num_offspring, dataset, early_stopping_generations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenetic Algorithm\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 73\u001b[0m     population \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_population\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_population\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitial Population:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36minitialize_population\u001b[0;34m(population_size, dataset)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_population\u001b[39m(population_size, dataset):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m----->Initializing Population\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     daddy \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m                                 \u001b[38;5;66;03m# load parent from input\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     population \u001b[38;5;241m=\u001b[39m [daddy]\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, population_size):\n","Cell \u001b[0;32mIn[3], line 20\u001b[0m, in \u001b[0;36mcompute_parent\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m daddy\n\u001b[1;32m     16\u001b[0m daddy \u001b[38;5;241m=\u001b[39m Network(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     19\u001b[0m layerList1 \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mConvolutional\u001b[49m(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, stride_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m),input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     21\u001b[0m     Convolutional(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, stride_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m),input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m3\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     22\u001b[0m     \n\u001b[1;32m     23\u001b[0m ]\n\u001b[1;32m     24\u001b[0m layerList2 \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     25\u001b[0m     Pooling(pool_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), stride_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m), padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msame\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     26\u001b[0m     Dropout(\u001b[38;5;241m0.25\u001b[39m)\n\u001b[1;32m     27\u001b[0m ]\n\u001b[1;32m     28\u001b[0m daddy\u001b[38;5;241m.\u001b[39mblock_list\u001b[38;5;241m.\u001b[39mappend(Block(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, layerList1, layerList2))\n","\u001b[0;31mNameError\u001b[0m: name 'Convolutional' is not defined"],"ename":"NameError","evalue":"name 'Convolutional' is not defined","output_type":"error"}],"execution_count":6},{"cell_type":"markdown","source":"## To remove a folder\n# Clear output folder\nimport os\n\ndef remove_folder_contents(folder):\n    for the_file in os.listdir(folder):\n        file_path = os.path.join(folder, the_file)\n        try:\n            if os.path.isfile(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                remove_folder_contents(file_path)\n                os.rmdir(file_path)\n        except Exception as e:\n            print(e)\n\nfolder_path = '/kaggle/working'\nremove_folder_contents(folder_path)\nos.rmdir(folder_path)","metadata":{"execution":{"iopub.status.busy":"2025-02-26T20:48:26.919941Z","iopub.execute_input":"2025-02-26T20:48:26.920268Z","iopub.status.idle":"2025-02-26T20:48:26.949259Z","shell.execute_reply.started":"2025-02-26T20:48:26.920243Z","shell.execute_reply":"2025-02-26T20:48:26.948047Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:15:11.525935Z","iopub.execute_input":"2025-02-26T20:15:11.526607Z","iopub.status.idle":"2025-02-26T20:15:11.551649Z","shell.execute_reply.started":"2025-02-26T20:15:11.526578Z","shell.execute_reply":"2025-02-26T20:15:11.550511Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32                       # the number of training examples in one forward/backward pass\nnum_classes = 10                        # number of cifar-10 dataset classes\nepochs =20   \ndataset = load_dataset(batch_size, num_classes, epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:18:14.960765Z","iopub.execute_input":"2025-02-26T20:18:14.961398Z","iopub.status.idle":"2025-02-26T20:18:16.106851Z","shell.execute_reply.started":"2025-02-26T20:18:14.961369Z","shell.execute_reply":"2025-02-26T20:18:16.106085Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Importing the necessary libraries, which we may or may not use. Its always good idea to import them befor (if you remember) else we can do it at any point of time no problem.\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D,MaxPool2D,Dense,Flatten,Dropout,Input, AveragePooling2D, Activation,Conv2D, MaxPooling2D, BatchNormalization,Concatenate\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard\nfrom tensorflow.keras import regularizers, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T12:29:09.965152Z","iopub.execute_input":"2025-02-27T12:29:09.965855Z","iopub.status.idle":"2025-02-27T12:29:21.679774Z","shell.execute_reply.started":"2025-02-27T12:29:09.965823Z","shell.execute_reply":"2025-02-27T12:29:21.678678Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from tensorflow.keras.datasets import cifar10\n\n(x_train, y_train), (x_test, y_test)=cifar10.load_data()\n\n\nfrom tensorflow.keras.utils import to_categorical\n\n# Normalizing\nx_train=x_train/255\nx_test=x_test/255\n\n#One hot encoding\ny_train_cat=to_categorical(y_train,10)\ny_test_cat=to_categorical(y_test,10)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T12:30:14.002747Z","iopub.execute_input":"2025-02-27T12:30:14.003360Z","iopub.status.idle":"2025-02-27T12:30:15.124209Z","shell.execute_reply.started":"2025-02-27T12:30:14.003329Z","shell.execute_reply":"2025-02-27T12:30:15.123147Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"dataset = {\n        'num_classes': 10,\n        'epochs': 20,\n        'x_train': x_train,\n        'y_train': y_train_cat,\n        'x_test': x_test,\n        'y_test': y_test_cat\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T12:35:21.570341Z","iopub.execute_input":"2025-02-27T12:35:21.571157Z","iopub.status.idle":"2025-02-27T12:35:21.575321Z","shell.execute_reply.started":"2025-02-27T12:35:21.571126Z","shell.execute_reply":"2025-02-27T12:35:21.574498Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"model3=Sequential()\nmodel3.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\nmodel3.add(MaxPool2D(pool_size=(2,2)))\nmodel3.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\nmodel3.add(MaxPool2D(pool_size=(2,2)))\nmodel3.add(Flatten())\nmodel3.add(Dense(256,activation='relu'))\nmodel3.add(Dense(10,activation='softmax'))\nmodel3.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n# training_steps = train_generator.samples//train_generator.batch_size\n# validation_steps=test_generator.samples//test_generator.batch_size\n# history=model1.fit_generator(train_generator,epochs=50,steps_per_epoch=training_steps,validation_data=test_generator,validation_steps=validation_steps,callbacks=[board])\nhistory1=model3.fit(dataset['x_train'],dataset['y_train'],epochs=20,validation_data=(dataset['x_test'],dataset['y_test']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T12:37:04.865040Z","iopub.execute_input":"2025-02-27T12:37:04.865846Z","iopub.status.idle":"2025-02-27T12:39:08.615284Z","shell.execute_reply.started":"2025-02-27T12:37:04.865813Z","shell.execute_reply":"2025-02-27T12:39:08.614416Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n1563/1563 [==============================] - 8s 4ms/step - loss: 1.5291 - accuracy: 0.4442 - val_loss: 1.2883 - val_accuracy: 0.5360\nEpoch 2/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.2234 - accuracy: 0.5649 - val_loss: 1.1449 - val_accuracy: 0.5948\nEpoch 3/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.0899 - accuracy: 0.6165 - val_loss: 1.1185 - val_accuracy: 0.6115\nEpoch 4/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9864 - accuracy: 0.6541 - val_loss: 1.0509 - val_accuracy: 0.6312\nEpoch 5/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9011 - accuracy: 0.6836 - val_loss: 1.0063 - val_accuracy: 0.6577\nEpoch 6/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.8278 - accuracy: 0.7093 - val_loss: 1.0575 - val_accuracy: 0.6494\nEpoch 7/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.7656 - accuracy: 0.7329 - val_loss: 0.9920 - val_accuracy: 0.6643\nEpoch 8/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.7056 - accuracy: 0.7526 - val_loss: 1.0289 - val_accuracy: 0.6739\nEpoch 9/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6507 - accuracy: 0.7714 - val_loss: 1.0245 - val_accuracy: 0.6668\nEpoch 10/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.5977 - accuracy: 0.7906 - val_loss: 1.1018 - val_accuracy: 0.6594\nEpoch 11/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.5440 - accuracy: 0.8085 - val_loss: 1.0921 - val_accuracy: 0.6703\nEpoch 12/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.4930 - accuracy: 0.8272 - val_loss: 1.1411 - val_accuracy: 0.6648\nEpoch 13/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.4457 - accuracy: 0.8418 - val_loss: 1.2065 - val_accuracy: 0.6658\nEpoch 14/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.4027 - accuracy: 0.8550 - val_loss: 1.2772 - val_accuracy: 0.6616\nEpoch 15/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.3622 - accuracy: 0.8718 - val_loss: 1.3390 - val_accuracy: 0.6570\nEpoch 16/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.3309 - accuracy: 0.8848 - val_loss: 1.4150 - val_accuracy: 0.6540\nEpoch 17/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2897 - accuracy: 0.8978 - val_loss: 1.5304 - val_accuracy: 0.6557\nEpoch 18/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2744 - accuracy: 0.9020 - val_loss: 1.5635 - val_accuracy: 0.6567\nEpoch 19/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2376 - accuracy: 0.9148 - val_loss: 1.7345 - val_accuracy: 0.6405\nEpoch 20/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2239 - accuracy: 0.9192 - val_loss: 1.7531 - val_accuracy: 0.6526\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"model1=Sequential()\nmodel1.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\nmodel1.add(MaxPool2D(pool_size=(2,2)))\nmodel1.add(Flatten())\nmodel1.add(Dense(256,activation='relu'))\nmodel1.add(Dense(10,activation='softmax'))\nmodel1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n# training_steps = train_generator.samples//train_generator.batch_size\n# validation_steps=test_generator.samples//test_generator.batch_size\n# history=model1.fit_generator(train_generator,epochs=50,steps_per_epoch=training_steps,validation_data=test_generator,validation_steps=validation_steps,callbacks=[board])\nhistory1=model1.fit(x_train,y_train_cat,epochs=20,validation_data=(x_test,y_test_cat))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-27T12:30:29.249623Z","iopub.execute_input":"2025-02-27T12:30:29.249991Z","iopub.status.idle":"2025-02-27T12:32:35.683236Z","shell.execute_reply.started":"2025-02-27T12:30:29.249966Z","shell.execute_reply":"2025-02-27T12:32:35.682451Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n1563/1563 [==============================] - 10s 4ms/step - loss: 1.4828 - accuracy: 0.4664 - val_loss: 1.2607 - val_accuracy: 0.5564\nEpoch 2/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.1579 - accuracy: 0.5910 - val_loss: 1.1321 - val_accuracy: 0.5977\nEpoch 3/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 1.0248 - accuracy: 0.6422 - val_loss: 1.0350 - val_accuracy: 0.6411\nEpoch 4/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.9196 - accuracy: 0.6791 - val_loss: 0.9818 - val_accuracy: 0.6638\nEpoch 5/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.8303 - accuracy: 0.7098 - val_loss: 0.9721 - val_accuracy: 0.6669\nEpoch 6/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.7530 - accuracy: 0.7365 - val_loss: 0.9590 - val_accuracy: 0.6777\nEpoch 7/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6837 - accuracy: 0.7606 - val_loss: 0.9564 - val_accuracy: 0.6849\nEpoch 8/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.6192 - accuracy: 0.7818 - val_loss: 1.0090 - val_accuracy: 0.6797\nEpoch 9/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.5594 - accuracy: 0.8038 - val_loss: 1.0112 - val_accuracy: 0.6912\nEpoch 10/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.5016 - accuracy: 0.8241 - val_loss: 1.0905 - val_accuracy: 0.6724\nEpoch 11/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.4497 - accuracy: 0.8406 - val_loss: 1.1456 - val_accuracy: 0.6726\nEpoch 12/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.3999 - accuracy: 0.8593 - val_loss: 1.2366 - val_accuracy: 0.6795\nEpoch 13/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.3606 - accuracy: 0.8721 - val_loss: 1.2978 - val_accuracy: 0.6665\nEpoch 14/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.3210 - accuracy: 0.8860 - val_loss: 1.3772 - val_accuracy: 0.6716\nEpoch 15/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2851 - accuracy: 0.8977 - val_loss: 1.4638 - val_accuracy: 0.6756\nEpoch 16/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2615 - accuracy: 0.9067 - val_loss: 1.5356 - val_accuracy: 0.6678\nEpoch 17/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2324 - accuracy: 0.9171 - val_loss: 1.6704 - val_accuracy: 0.6705\nEpoch 18/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.2083 - accuracy: 0.9258 - val_loss: 1.8549 - val_accuracy: 0.6665\nEpoch 19/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.1923 - accuracy: 0.9294 - val_loss: 1.9337 - val_accuracy: 0.6577\nEpoch 20/20\n1563/1563 [==============================] - 6s 4ms/step - loss: 0.1823 - accuracy: 0.9358 - val_loss: 1.9804 - val_accuracy: 0.6604\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model2=Sequential()\nmodel2.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(Dropout(0.25)) # Drop 25% of the units from the layer.\nmodel2.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(32,32,3),activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2,2)))\nmodel2.add(Dropout(0.25))\nmodel2.add(Flatten())\nmodel2.add(Dense(256,activation='relu'))\nmodel2.add(Dense(10,activation='softmax'))\nmodel2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n# training_steps = train_generator.samples//train_generator.batch_size\n# validation_steps=test_generator.samples//test_generator.batch_size\n# model2.fit_generator(train_generator,epochs=100,steps_per_epoch=training_steps,validation_data=test_generator,validation_steps=validation_steps,callbacks=[board])\nhistory2=model2.fit(dataset['x_train'],dataset['y_train'],epochs=20,validation_data=(dataset['x_test'],dataset['y_test']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:29:58.115741Z","iopub.execute_input":"2025-02-26T20:29:58.116083Z","iopub.status.idle":"2025-02-26T20:32:04.755572Z","shell.execute_reply.started":"2025-02-26T20:29:58.116056Z","shell.execute_reply":"2025-02-26T20:32:04.754849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\ndef load_dataset(batch_size, num_classes, epochs):\n    # Load CIFAR-10 dataset\n    (x_train_full, y_train_full), (x_test, y_test) = cifar10.load_data()\n\n    # Normalize images (convert to float32 and scale to [0,1])\n    x_train_full = x_train_full.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\n\n\n    print('Shape of x_train is {}'.format(x_train_full.shape))\n    print('Shape of x_test is {}'.format(x_test.shape)) \n    print('Shape of y_train is {}'.format(y_train_full.shape))\n    print('Shape of y_test is {}'.format(y_test.shape))\n    \n    # Convert labels to one-hot encoding\n    y_train_full = to_categorical(y_train_full, num_classes)\n    y_test = to_categorical(y_test, num_classes)\n\n    print('Shape of y_train is {}'.format(y_train_full.shape))\n    print('Shape of y_test is {}'.format(y_test.shape))\n    \n    # Split training data into 40,000 for training and 10,000 for validation\n    x_train, x_val, y_train, y_val = train_test_split(\n        x_train_full, y_train_full, test_size=0.2, random_state=42\n    )\n\n    dataset = {\n        'batch_size': batch_size,\n        'num_classes': num_classes,\n        'epochs': epochs,\n        'x_train': x_train,\n        'y_train': y_train,\n        'x_val': x_val,\n        'y_val': y_val,\n        'x_test': x_test,  \n        'y_test': y_test\n    }\n\n    return dataset\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:40:10.450604Z","iopub.execute_input":"2025-02-26T20:40:10.451349Z","iopub.status.idle":"2025-02-26T20:40:10.458377Z","shell.execute_reply.started":"2025-02-26T20:40:10.451320Z","shell.execute_reply":"2025-02-26T20:40:10.457456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 32                       # the number of training examples in one forward/backward pass\nnum_classes = 10                        # number of cifar-10 dataset classes\nepochs =20   \ndataset = load_dataset(batch_size, num_classes, epochs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T20:40:11.115033Z","iopub.execute_input":"2025-02-26T20:40:11.115395Z","iopub.status.idle":"2025-02-26T20:40:12.374861Z","shell.execute_reply.started":"2025-02-26T20:40:11.115367Z","shell.execute_reply":"2025-02-26T20:40:12.373960Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}